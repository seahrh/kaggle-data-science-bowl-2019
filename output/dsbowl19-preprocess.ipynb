{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from math import sqrt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log(str):\n",
    "    os.system(f'echo \\\"{str}\\\"')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN = '__NAN__'\n",
    "INPUT_ROOT = '../input/data-science-bowl-2019'\n",
    "JOIN_KEY = ['installation_id', 'game_session', 'title']\n",
    "TARGET = 'accuracy_group'\n",
    "FEATURES = {\n",
    "    'event_id', \n",
    "    'game_session', \n",
    "    'timestamp', \n",
    "    'installation_id', \n",
    "    'event_count',\n",
    "    'event_code', \n",
    "    'game_time', \n",
    "    'title', \n",
    "    'type', \n",
    "    'world',\n",
    "    'event_data'\n",
    "}\n",
    "EVENT_CODES = ['2000', '2010', '2020', '2025', '2030', '2035', '2040', '2050', '2060', '2070', '2075', '2080', '2081', '2083', '3010', '3020', '3021', '3110', '3120', '3121', '4010', '4020', '4021', '4022', '4025', '4030', '4031', '4035', '4040', '4045', '4050', '4070', '4080', '4090', '4095', '4100', '4110', '4220', '4230', '4235', '5000', '5010']\n",
    "SEED = 31\n",
    "FOLDS = 3\n",
    "ESTIMATORS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init():\n",
    "    # Characters such as empty strings '' or numpy.inf are considered NA values\n",
    "    pd.set_option('use_inf_as_na', True)\n",
    "    pd.set_option('display.max_columns', 999)\n",
    "    pd.set_option('display.max_rows', 999)\n",
    "    \n",
    "    \n",
    "_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/data-science-bowl-2019\\sample_submission.csv\n",
      "../input/data-science-bowl-2019\\test.csv\n",
      "../input/data-science-bowl-2019\\test.csv.zip\n",
      "../input/data-science-bowl-2019\\train.csv\n",
      "../input/data-science-bowl-2019\\train.csv.zip\n",
      "../input/data-science-bowl-2019\\train_labels.csv\n",
      "../input/data-science-bowl-2019\\train_labels.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_ROOT):\n",
    "    for filename in filenames:\n",
    "        _log(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_raw = pd.read_csv(f'{INPUT_ROOT}/train.csv', usecols=FEATURES)\n",
    "train_labels = pd.read_csv(f'{INPUT_ROOT}/train_labels.csv', usecols=JOIN_KEY + [TARGET])\n",
    "test_raw = pd.read_csv(f'{INPUT_ROOT}/test.csv', usecols=FEATURES)\n",
    "train_labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_unlabelled_data(train_raw, train_labels):\n",
    "    return train_raw[train_raw['installation_id'].isin(train_labels['installation_id'].unique())]\n",
    "\n",
    "\n",
    "train_raw = _remove_unlabelled_data(train_raw, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _add_labels(train_raw, train_labels, on):\n",
    "    return pd.merge(train_raw, train_labels, on=on, how='left')\n",
    "\n",
    "\n",
    "train_raw = _add_labels(train_raw, train_labels, on=JOIN_KEY)\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract event data JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_columns(df1, df2):\n",
    "    \"\"\"Concatenate the columns of two pandas dataframes in the order of the operands.\n",
    "    Both dataframes must have the same number of rows.\n",
    "    \"\"\"\n",
    "    assert len(df1) == len(df2)\n",
    "    res = pd.concat([df1, df2.reindex(df1.index)], axis=1, join='inner')\n",
    "    assert len(res) == len(df1)\n",
    "    return res\n",
    "    \n",
    "\n",
    "def _extract_event_data(df, keep_cols, chunk_size=1000000):\n",
    "    res = pd.DataFrame()\n",
    "    _len = len(df)\n",
    "    for i in tqdm(range(0, _len, chunk_size)):\n",
    "        if i + chunk_size < _len:\n",
    "            chunk = df[i:i + chunk_size].copy()\n",
    "        else:\n",
    "            chunk = df[i:].copy()\n",
    "        ed = pd.io.json.json_normalize(chunk['event_data'].apply(json.loads)).add_prefix('ed.')\n",
    "        ed = ed[keep_cols]\n",
    "        chunk = _concat_columns(chunk, ed)\n",
    "        res = pd.concat([res, chunk], ignore_index=True, sort=False)\n",
    "    assert len(df) == len(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "keep_cols = ['ed.identifier', 'ed.duration', 'ed.level', 'ed.round', 'ed.correct', 'ed.misses',\n",
    "            'ed.weight', 'ed.total_duration', 'ed.source']\n",
    "train_raw = _extract_event_data(train_raw, keep_cols)\n",
    "test_raw = _extract_event_data(test_raw, keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All event ids in test set also exist in train set\n",
    "test_set = set(test_raw['event_id'])\n",
    "train_set = set(train_raw['event_id'])\n",
    "vs = test_set - train_set\n",
    "_log(f'{len(vs)} event_ids exist in test set but not train set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_IDS = sorted(test_raw['event_id'].unique())\n",
    "_log(f'{len(vs)} EVENT_IDS={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(train_raw['type'].unique())\n",
    "_log(f'{len(vs)} train_raw type={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(train_raw['world'].unique())\n",
    "_log(f'{len(vs)} train_raw type={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(train_raw['event_code'].unique())\n",
    "_log(f'{len(vs)} train_raw type={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(train_raw['title'].unique())\n",
    "_log(f'{len(vs)} train_raw titles={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(test_raw['title'].unique())\n",
    "_log(f'{len(vs)} test titles={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_timestamp(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "\n",
    "train_raw = _transform_timestamp(train_raw)\n",
    "test_raw = _transform_timestamp(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _set_string_type(df, cols):\n",
    "    df[cols] = df[cols].fillna(NAN).astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "cols = ['event_code', 'timestamp']\n",
    "train_raw = _set_string_type(train_raw, cols=cols)\n",
    "test_raw = _set_string_type(test_raw, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _sort_it(df):\n",
    "    return df.sort_values(by=['installation_id', 'timestamp'])\n",
    "\n",
    "\n",
    "train_raw = _sort_it(train_raw)\n",
    "test_raw = _sort_it(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple accuracy groups per installation id\n",
    "In the train set, there are multiple accuracy groups per installation id. The task is to predict the accuracy group of the **last** assessment for a given installation id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = train_raw[train_raw[TARGET].notna()].groupby('installation_id')[TARGET].nunique()\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_smoothing(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = np.log(df[col] + 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "#cols = ['event_count', 'game_time']\n",
    "#train = _log_smoothing(train, cols)\n",
    "#test = _log_smoothing(test, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split not by time\n",
    "Both train and test sets span the same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log(f'train[timestamp] is from {train_raw.timestamp.min()} to {train_raw.timestamp.max()}')\n",
    "_log(f'test[timestamp] is from {test_raw.timestamp.min()} to {test_raw.timestamp.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cutoff(df, TARGET):\n",
    "    return df[df[TARGET].notna()]['timestamp'].max()\n",
    "\n",
    "\n",
    "def _trim_events_after_last_assessment(df, cutoff):\n",
    "    res = df[df['timestamp'] <= cutoff]\n",
    "    #_log(f'cutoff: before={df.shape}, after={res.shape}')\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def _target_variable(df, cutoff, TARGET):\n",
    "    vs = df[df['timestamp'] == cutoff][TARGET].values\n",
    "    assert len(set(vs)) == 1\n",
    "    return int(float(vs[0]))\n",
    "    \n",
    "    \n",
    "def _game_session_stats(df, col):\n",
    "    vs = df.groupby(['game_session'])[col].transform('max')\n",
    "    return (vs.median(), vs.max())\n",
    "\n",
    "\n",
    "def _event_code_counts(df, code):\n",
    "    total = np.int32([sum(df['event_code'] == code)])\n",
    "    activity = np.int32([sum((df['event_code'] == code) & (df['type'] == 'Activity'))])\n",
    "    assessment = np.int32([sum((df['event_code'] == code) & (df['type'] == 'Assessment'))])\n",
    "    clip = np.int32([sum((df['event_code'] == code) & (df['type'] == 'Clip'))])\n",
    "    game = np.int32([sum((df['event_code'] == code) & (df['type'] == 'Game'))])\n",
    "    return (total, activity, assessment, clip, game)\n",
    "\n",
    "\n",
    "def _event_data_features(df, suffix):\n",
    "    res = {}\n",
    "    res[f'ed_duration{suffix}'] = np.int32(df['ed.duration'].fillna(0).max())\n",
    "    res[f'ed_total_duration{suffix}'] = np.int32(df['ed.total_duration'].fillna(0).max())\n",
    "    res[f'ed_level{suffix}'] = np.int32(df['ed.level'].fillna(0).max())\n",
    "    res[f'ed_round{suffix}'] = np.int32(df['ed.round'].fillna(0).max())\n",
    "    res[f'ed_correct{suffix}'] = np.int32(df['ed.correct'].fillna(0).max())\n",
    "    res[f'ed_misses{suffix}'] = np.int32(df['ed.misses'].fillna(0).max())\n",
    "    res[f'ed_weight{suffix}'] = np.int32(df['ed.weight'].fillna(0).max())\n",
    "    return res\n",
    "\n",
    "\n",
    "def _features_map(df, EVENT_CODES, EVENT_IDS, suffix=''):\n",
    "    res = {}\n",
    "    res[f'type_activity{suffix}'] = np.int32([sum(df['type'] == 'Activity')])\n",
    "    res[f'type_assessment{suffix}'] = np.int32([sum(df['type'] == 'Assessment')])\n",
    "    res[f'type_clip{suffix}'] = np.int32([sum(df['type'] == 'Clip')])\n",
    "    res[f'type_game{suffix}'] = np.int32([sum(df['type'] == 'Game')])\n",
    "    assert len(df) == res[f'type_activity{suffix}'][0] + res[f'type_assessment{suffix}'][0] + res[f'type_clip{suffix}'][0] + res[f'type_game{suffix}'][0]\n",
    "    res[f'world_crystalcaves{suffix}'] = np.int32([sum(df['world'] == 'CRYSTALCAVES')])\n",
    "    res[f'world_magmapeak{suffix}'] = np.int32([sum(df['world'] == 'MAGMAPEAK')])\n",
    "    res[f'world_treetopcity{suffix}'] = np.int32([sum(df['world'] == 'TREETOPCITY')])\n",
    "    res[f'world_none{suffix}'] = np.int32([sum(df['world'] == 'NONE')])\n",
    "    res[f'title_12_monkeys{suffix}'] = np.int32([sum(df['title'] == '12 Monkeys')])\n",
    "    res[f'title_air_show{suffix}'] = np.int32([sum(df['title'] == 'Air Show')])\n",
    "    res[f'title_all_star_sorting{suffix}'] = np.int32([sum(df['title'] == 'All Star Sorting')])\n",
    "    res[f'title_balancing_act{suffix}'] = np.int32([sum(df['title'] == 'Balancing Act')])\n",
    "    res[f'title_bird_measurer{suffix}'] = np.int32([sum(df['title'] == 'Bird Measurer (Assessment)')])\n",
    "    res[f'title_bottle_filler{suffix}'] = np.int32([sum(df['title'] == 'Bottle Filler (Activity)')])\n",
    "    res[f'title_bubble_bath{suffix}'] = np.int32([sum(df['title'] == 'Bubble Bath')])\n",
    "    res[f'title_bug_measurer{suffix}'] = np.int32([sum(df['title'] == 'Bug Measurer (Activity)')])\n",
    "    res[f'title_cart_balancer{suffix}'] = np.int32([sum(df['title'] == 'Cart Balancer (Assessment)')])\n",
    "    res[f'title_cauldron_filler{suffix}'] = np.int32([sum(df['title'] == 'Cauldron Filler (Assessment)')])\n",
    "    res[f'title_chest_sorter{suffix}'] = np.int32([sum(df['title'] == 'Chest Sorter (Assessment)')])\n",
    "    res[f'title_chicken_balancer{suffix}'] = np.int32([sum(df['title'] == 'Chicken Balancer (Activity)')])\n",
    "    res[f'title_chow_time{suffix}'] = np.int32([sum(df['title'] == 'Chow Time')])\n",
    "    res[f'title_costume_box{suffix}'] = np.int32([sum(df['title'] == 'Costume Box')])\n",
    "    res[f'title_crystal_caves_1{suffix}'] = np.int32([sum(df['title'] == 'Crystal Caves - Level 1')])\n",
    "    res[f'title_crystal_caves_2{suffix}'] = np.int32([sum(df['title'] == 'Crystal Caves - Level 2')])\n",
    "    res[f'title_crystal_caves_3{suffix}'] = np.int32([sum(df['title'] == 'Crystal Caves - Level 3')])\n",
    "    res[f'title_crystals_rule{suffix}'] = np.int32([sum(df['title'] == 'Crystals Rule')])\n",
    "    res[f'title_dino_dive{suffix}'] = np.int32([sum(df['title'] == 'Dino Dive')])\n",
    "    res[f'title_dino_drink{suffix}'] = np.int32([sum(df['title'] == 'Dino Drink')])\n",
    "    res[f'title_egg_dropper{suffix}'] = np.int32([sum(df['title'] == 'Egg Dropper (Activity)')])\n",
    "    res[f'title_fireworks{suffix}'] = np.int32([sum(df['title'] == 'Fireworks (Activity)')])\n",
    "    res[f'title_flower_waterer{suffix}'] = np.int32([sum(df['title'] == 'Flower Waterer (Activity)')])\n",
    "    res[f'title_happy_camel{suffix}'] = np.int32([sum(df['title'] == 'Happy Camel')])\n",
    "    res[f'title_heavy{suffix}'] = np.int32([sum(df['title'] == 'Heavy, Heavier, Heaviest')])\n",
    "    res[f'title_honey_cake{suffix}'] = np.int32([sum(df['title'] == 'Honey Cake')])\n",
    "    res[f'title_leaf_leader{suffix}'] = np.int32([sum(df['title'] == 'Leaf Leader')])\n",
    "    res[f'title_lifting{suffix}'] = np.int32([sum(df['title'] == 'Lifting Heavy Things')])\n",
    "    res[f'title_magma_peak_1{suffix}'] = np.int32([sum(df['title'] == 'Magma Peak - Level 1')])\n",
    "    res[f'title_magma_peak_2{suffix}'] = np.int32([sum(df['title'] == 'Magma Peak - Level 2')])\n",
    "    res[f'title_mushroom_sorter{suffix}'] = np.int32([sum(df['title'] == 'Mushroom Sorter (Assessment)')])\n",
    "    res[f'title_ordering_spheres{suffix}'] = np.int32([sum(df['title'] == 'Ordering Spheres')])\n",
    "    res[f'title_pan_balance{suffix}'] = np.int32([sum(df['title'] == 'Pan Balance')])\n",
    "    res[f'title_pirate_tale{suffix}'] = np.int32([sum(df['title'] == \"Pirate's Tale\")])\n",
    "    res[f'title_rulers{suffix}'] = np.int32([sum(df['title'] == 'Rulers')])\n",
    "    res[f'title_sandcastle{suffix}'] = np.int32([sum(df['title'] == 'Sandcastle Builder (Activity)')])\n",
    "    res[f'title_scrub{suffix}'] = np.int32([sum(df['title'] == 'Scrub-A-Dub')])\n",
    "    res[f'title_slop{suffix}'] = np.int32([sum(df['title'] == 'Slop Problem')])\n",
    "    res[f'title_treasure_map{suffix}'] = np.int32([sum(df['title'] == 'Treasure Map')])\n",
    "    res[f'title_treetop_city_1{suffix}'] = np.int32([sum(df['title'] == 'Tree Top City - Level 1')])\n",
    "    res[f'title_treetop_city_2{suffix}'] = np.int32([sum(df['title'] == 'Tree Top City - Level 2')])\n",
    "    res[f'title_treetop_city_3{suffix}'] = np.int32([sum(df['title'] == 'Tree Top City - Level 3')])\n",
    "    res[f'title_watering_hole{suffix}'] = np.int32([sum(df['title'] == 'Watering Hole (Activity)')])\n",
    "    res[f'title_welcome{suffix}'] = np.int32([sum(df['title'] == 'Welcome to Lost Lagoon!')])\n",
    "    \n",
    "    for code in EVENT_CODES:\n",
    "        (total, activity, assessment, clip, game) = _event_code_counts(df, code)\n",
    "        res[f'event_{code}{suffix}'] = total\n",
    "        res[f'event_{code}_activity{suffix}'] = activity\n",
    "        res[f'event_{code}_assessment{suffix}'] = assessment\n",
    "        res[f'event_{code}_clip{suffix}'] = clip\n",
    "        res[f'event_{code}_game{suffix}'] = game\n",
    "    \n",
    "    for eid in EVENT_IDS:\n",
    "        res[f'eid_{eid}{suffix}'] = np.int32([sum(df['event_id'] == eid)])\n",
    "    \n",
    "    res[f'game_time{suffix}'] = np.int32(df['game_time'].max())\n",
    "    res[f'event_count{suffix}'] = np.int32(df['event_count'].max())\n",
    "    res.update(_event_data_features(df, suffix))\n",
    "    return res\n",
    "\n",
    "\n",
    "def _features(df, installation_id, EVENT_CODES, EVENT_IDS):\n",
    "    res = {}\n",
    "    iid = df[df['installation_id'] == installation_id].copy()\n",
    "    if TARGET in df.columns:\n",
    "        cutoff = _cutoff(iid, TARGET)\n",
    "        iid = _trim_events_after_last_assessment(iid, cutoff)\n",
    "        res[TARGET] = _target_variable(iid, cutoff, TARGET)\n",
    "    res['installation_id'] = [installation_id]\n",
    "    cols = ['game_time', 'event_count']\n",
    "    for col in cols:\n",
    "        (_median, _max) = np.int32(_game_session_stats(iid, col))\n",
    "        res[f'{col}_p50'] = _median\n",
    "        res[f'{col}_max'] = _max\n",
    "    res.update(_features_map(iid, EVENT_CODES, EVENT_IDS))\n",
    "    return pd.DataFrame.from_dict(res)\n",
    "\n",
    "\n",
    "def _preprocess(raw, EVENT_CODES, EVENT_IDS):\n",
    "    res = pd.DataFrame()\n",
    "    iids = raw['installation_id'].unique()\n",
    "    for iid in tqdm(iids):\n",
    "        res = pd.concat([res, _features(raw, iid, EVENT_CODES, EVENT_IDS)], ignore_index=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "train = _preprocess(train_raw, EVENT_CODES, EVENT_IDS)\n",
    "train.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _preprocess(test_raw, EVENT_CODES, EVENT_IDS)\n",
    "test.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('train.parquet')\n",
    "test.to_parquet('test.parquet')\n",
    "_log(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTORS = set(test.columns.values) - {'installation_id'}\n",
    "#PREDICTORS = set(test.columns.values) - {'installation_id', 'ed_duration','ed_total_duration','ed_level','ed_round','ed_correct','ed_misses','ed_weight'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(y, y_pred):\n",
    "    return sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "SCORING = make_scorer(_rmse, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train = train[TARGET].astype(int)\n",
    "x_train = train[PREDICTORS]\n",
    "model = lgb.LGBMRegressor(n_estimators=ESTIMATORS, reg_alpha=1)\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.01],\n",
    "    'model__num_leaves': [80],\n",
    "    'model__min_child_samples': [200],\n",
    "    'model__colsample_bytree': [0.5]\n",
    "}\n",
    "cv = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring=SCORING)\n",
    "#cv.fit(x_train, y_train, model__early_stopping_rounds=200, model__verbose=500)\n",
    "cv.fit(x_train, y_train)\n",
    "#assert cv.best_estimator_['model'].n_classes_ == 4\n",
    "_log(f'best_params_={cv.best_params_}\\nbest_score_={cv.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metric only works with early stopping rounds\n",
    "#lgb.plot_metric(cv.best_estimator_['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(cv.best_estimator_['model'], max_num_features=100, figsize=(10, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict out of fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = train[['installation_id']].copy()\n",
    "oof[TARGET] = cv.predict(x_train)\n",
    "assert oof[TARGET].min() > -0.5\n",
    "assert oof[TARGET].max() < 3.5\n",
    "oof[TARGET] = np.round(oof[TARGET]).astype(int)\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cohen_kappa_score(oof[TARGET], y_train, weights='quadratic')\n",
    "_log(f'oof score={score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[PREDICTORS]\n",
    "sub = test[['installation_id']].copy()\n",
    "sub[TARGET] = cv.predict(x_test)\n",
    "assert sub[TARGET].min() > -0.5\n",
    "assert sub[TARGET].max() < 3.5\n",
    "sub[TARGET] = np.round(sub[TARGET]).astype(int)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.title('test predict')\n",
    "sub[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('oof predict')\n",
    "oof[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('oof truth')\n",
    "tmp = train[TARGET].copy()\n",
    "tmp = tmp.astype(int)\n",
    "tmp.plot(kind='hist')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)\n",
    "_log(os.listdir(\".\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
