{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit           : None\n",
      "python           : 3.7.1.final.0\n",
      "python-bits      : 64\n",
      "OS               : Windows\n",
      "OS-release       : 7\n",
      "machine          : AMD64\n",
      "processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "byteorder        : little\n",
      "LC_ALL           : None\n",
      "LANG             : None\n",
      "LOCALE           : None.None\n",
      "\n",
      "pandas           : 0.25.3\n",
      "numpy            : 1.17.4\n",
      "pytz             : 2019.3\n",
      "dateutil         : 2.8.1\n",
      "pip              : 19.3.1\n",
      "setuptools       : 39.0.1\n",
      "Cython           : None\n",
      "pytest           : None\n",
      "hypothesis       : None\n",
      "sphinx           : None\n",
      "blosc            : None\n",
      "feather          : None\n",
      "xlsxwriter       : None\n",
      "lxml.etree       : None\n",
      "html5lib         : None\n",
      "pymysql          : None\n",
      "psycopg2         : None\n",
      "jinja2           : 2.10.3\n",
      "IPython          : 7.10.2\n",
      "pandas_datareader: None\n",
      "bs4              : None\n",
      "bottleneck       : None\n",
      "fastparquet      : None\n",
      "gcsfs            : None\n",
      "lxml.etree       : None\n",
      "matplotlib       : 3.1.2\n",
      "numexpr          : None\n",
      "odfpy            : None\n",
      "openpyxl         : None\n",
      "pandas_gbq       : None\n",
      "pyarrow          : 0.15.1\n",
      "pytables         : None\n",
      "s3fs             : None\n",
      "scipy            : 1.4.1\n",
      "sqlalchemy       : None\n",
      "tables           : None\n",
      "xarray           : None\n",
      "xlrd             : None\n",
      "xlwt             : None\n",
      "xlsxwriter       : None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from math import sqrt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log(str):\n",
    "    os.system(f'echo \\\"{str}\\\"')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN = '__NAN__'\n",
    "INPUT_ROOT = '../input/data-science-bowl-2019'\n",
    "JOIN_KEY = ['installation_id', 'game_session', 'title']\n",
    "TARGET = 'accuracy_group'\n",
    "FEATURES = {\n",
    "    'event_id', \n",
    "    'game_session', \n",
    "    'timestamp', \n",
    "    'installation_id', \n",
    "    'event_count',\n",
    "    'event_code', \n",
    "    'game_time', \n",
    "    'title', \n",
    "    'type', \n",
    "    'world',\n",
    "    'event_data'\n",
    "}\n",
    "EVENT_CODES = ['2000', '2010', '2020', '2025', '2030', '2035', '2040', '2050', '2060', '2070', '2075', '2080', '2081', '2083', '3010', '3020', '3021', '3110', '3120', '3121', '4010', '4020', '4021', '4022', '4025', '4030', '4031', '4035', '4040', '4045', '4050', '4070', '4080', '4090', '4095', '4100', '4110', '4220', '4230', '4235', '5000', '5010']\n",
    "SEED = 31\n",
    "FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init():\n",
    "    # Characters such as empty strings '' or numpy.inf are considered NA values\n",
    "    pd.set_option('use_inf_as_na', True)\n",
    "    pd.set_option('display.max_columns', 999)\n",
    "    pd.set_option('display.max_rows', 999)\n",
    "    \n",
    "    \n",
    "_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/data-science-bowl-2019\\sample_submission.csv\n",
      "../input/data-science-bowl-2019\\test.csv\n",
      "../input/data-science-bowl-2019\\test.csv.zip\n",
      "../input/data-science-bowl-2019\\train.csv\n",
      "../input/data-science-bowl-2019\\train.csv.zip\n",
      "../input/data-science-bowl-2019\\train_labels.csv\n",
      "../input/data-science-bowl-2019\\train_labels.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_ROOT):\n",
    "    for filename in filenames:\n",
    "        _log(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17690 entries, 0 to 17689\n",
      "Data columns (total 4 columns):\n",
      "game_session       17690 non-null object\n",
      "installation_id    17690 non-null object\n",
      "title              17690 non-null object\n",
      "accuracy_group     17690 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 552.9+ KB\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_raw = pd.read_csv(f'{INPUT_ROOT}/train.csv', usecols=FEATURES)\n",
    "train_labels = pd.read_csv(f'{INPUT_ROOT}/train_labels.csv', usecols=JOIN_KEY + [TARGET])\n",
    "test_raw = pd.read_csv(f'{INPUT_ROOT}/test.csv', usecols=FEATURES)\n",
    "train_labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_unlabelled_data(train_raw, train_labels):\n",
    "    return train_raw[train_raw['installation_id'].isin(train_labels['installation_id'].unique())]\n",
    "\n",
    "\n",
    "train_raw = _remove_unlabelled_data(train_raw, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _add_labels(train_raw, train_labels, on):\n",
    "    return pd.merge(train_raw, train_labels, on=on, how='left')\n",
    "\n",
    "\n",
    "train_raw = _add_labels(train_raw, train_labels, on=JOIN_KEY)\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract event data JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_columns(df1, df2):\n",
    "    \"\"\"Concatenate the columns of two pandas dataframes in the order of the operands.\n",
    "    Both dataframes must have the same number of rows.\n",
    "    \"\"\"\n",
    "    assert len(df1) == len(df2)\n",
    "    res = pd.concat([df1, df2.reindex(df1.index)], axis=1, join='inner')\n",
    "    assert len(res) == len(df1)\n",
    "    return res\n",
    "    \n",
    "\n",
    "def _extract_event_data(df, keep_cols, chunk_size=1000000):\n",
    "    res = pd.DataFrame()\n",
    "    _len = len(df)\n",
    "    for i in tqdm(range(0, _len, chunk_size)):\n",
    "        if i + chunk_size < _len:\n",
    "            chunk = df[i:i + chunk_size].copy()\n",
    "        else:\n",
    "            chunk = df[i:].copy()\n",
    "        ed = pd.io.json.json_normalize(chunk['event_data'].apply(json.loads)).add_prefix('ed.')\n",
    "        ed = ed[keep_cols]\n",
    "        chunk = _concat_columns(chunk, ed)\n",
    "        res = pd.concat([res, chunk], ignore_index=True, sort=False)\n",
    "    assert len(df) == len(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "#keep_cols = ['ed.identifier', 'ed.duration', 'ed.level', 'ed.round', 'ed.correct', 'ed.misses',\n",
    "#            'ed.weight', 'ed.total_duration', 'ed.source']\n",
    "#train_raw = _extract_event_data(train_raw, keep_cols)\n",
    "#test_raw = _extract_event_data(test_raw, keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1156414 entries, 0 to 1156413\n",
      "Data columns (total 11 columns):\n",
      "event_id           1156414 non-null object\n",
      "game_session       1156414 non-null object\n",
      "timestamp          1156414 non-null object\n",
      "event_data         1156414 non-null object\n",
      "installation_id    1156414 non-null object\n",
      "event_count        1156414 non-null int64\n",
      "event_code         1156414 non-null int64\n",
      "game_time          1156414 non-null int64\n",
      "title              1156414 non-null object\n",
      "type               1156414 non-null object\n",
      "world              1156414 non-null object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 97.1+ MB\n"
     ]
    }
   ],
   "source": [
    "test_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7734558 entries, 0 to 7734557\n",
      "Data columns (total 12 columns):\n",
      "event_id           object\n",
      "game_session       object\n",
      "timestamp          object\n",
      "event_data         object\n",
      "installation_id    object\n",
      "event_count        int64\n",
      "event_code         int64\n",
      "game_time          int64\n",
      "title              object\n",
      "type               object\n",
      "world              object\n",
      "accuracy_group     float64\n",
      "dtypes: float64(1), int64(3), object(8)\n",
      "memory usage: 767.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 event_ids exist in test set but not train set.\n"
     ]
    }
   ],
   "source": [
    "# All event ids in test set also exist in train set\n",
    "test_set = set(test_raw['event_id'])\n",
    "train_set = set(train_raw['event_id'])\n",
    "vs = test_set - train_set\n",
    "_log(f'{len(vs)} event_ids exist in test set but not train set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 EVENT_IDS=['0086365d', '00c73085', '01ca3a3c', '022b4259', '02a42007', '0330ab6a', '0413e89d', '04df9b66', '05ad839b', '06372577', '070a5291', '08fd73f3', '08ff79ad', '0a08139c', '0ce40006', '0d18d96c', '0d1da71f', '0db6d71d', '119b5b02', '1325467d', '1340b8d7', '1375ccb7', '13f56524', '14de4c5d', '155f62a4', '1575e76c', '15a43e5b', '15ba1109', '15eb4a7d', '15f99afc', '160654fd', '16667cc5', '16dffff1', '17113b36', '19967db1', '1996c610', '1af8be29', '1b54d27f', '1bb5fbdb', '1beb320a', '1c178d24', '1cc7cfca', '1cf54632', '1f19558b', '222660ff', '2230fab4', '250513af', '25fa8af4', '262136f4', '26a5a3dd', '26fd2d99', '27253bdc', '28520915', '28a4eb9a', '28ed704e', '28f975ea', '29a42aea', '29bdd9ba', '29f54413', '2a444e03', '2a512369', '2b058fe3', '2b9272f4', '2c4e6db0', '2dc29e21', '2dcad279', '2ec694de', '2fb91ec1', '30614231', '30df3273', '31973d56', '3323d7e9', '33505eae', '3393b68b', '363c86c9', '363d3849', '36fa3ebe', '37937459', '37c53127', '37db1c2f', '37ee8496', '38074c54', '392e14df', '3a4be871', '3afb49e6', '3afde5dd', '3b2048ee', '3babcb9b', '3bb91ced', '3bb91dda', '3bf1cf26', '3bfd1a65', '3ccd3f02', '3d0b9317', '3d63345e', '3d8c61b0', '3dcdda7f', '3ddc79c3', '3dfd4aa4', '3edf6747', '3ee399c3', '44cb4907', '45d01abe', '461eace6', '46b50ba8', '46cd75b4', '47026d5f', '47efca07', '47f43a44', '48349b14', '4901243f', '499edb7c', '49ed92e9', '4a09ace1', '4a4c3d21', '4b5efe37', '4bb2f698', '4c2ec19f', '4d6737eb', '4d911100', '4e5fc6f5', '4ef8cdd3', '51102b85', '51311d7a', '5154fc30', '5290eab1', '532a2afb', '5348fd84', '53c6e11a', '55115cbd', '562cec5f', '565a3990', '56817e2b', '56bcd38d', '56cd3b43', '5859dfb6', '587b5989', '58a0de5c', '598f4598', '5a848010', '5b49460a', '5be391b5', '5c2f29ca', '5c3d2b2f', '5d042115', '5de79a6a', '5e109ec3', '5e3ea25a', '5e812b27', '5f0eb72c', '5f5b2617', '6043a2b4', '6077cc36', '6088b756', '611485c5', '63f13dd7', '65a38bf7', '65abac75', '67439901', '67aa2ada', '69fdac0a', '6aeafed4', '6bf9e3e1', '6c517a88', '6c930e6e', '6cf7d25c', '6d90d394', '6f445b57', '6f4adc4b', '6f4bd64e', '6f8106d9', '7040c096', '709b1251', '71e712d8', '71fe8f75', '731c0cbe', '736f9581', '7372e1a5', '73757a5e', '7423acbc', '74e5f8a7', '7525289a', '756e5507', '763fc34e', '76babcde', '77261ab5', '77c76bc5', '77ead60d', '792530f8', '795e4a37', '7961e599', '7ab78247', '7ad3efc6', '7cf1bc53', '7d093bf9', '7d5c30a2', '7da34a02', '7dfe6d8a', '7ec0c298', '7f0836bf', '7fd1ac25', '804ee27f', '828e68f9', '832735e1', '83c6c409', '84538528', '84b0e0c8', '857f21c0', '85d1b0de', '85de926c', '86ba578b', '86c924c4', '87d743c1', '884228c8', '88d4a5be', '895865f3', '89aace00', '8ac7cce4', '8af75982', '8b757ab8', '8d748b58', '8d7e386c', '8d84fa81', '8f094001', '8fee50e2', '907a054b', '90d848e0', '90ea0bac', '90efca10', '91561152', '923afab1', '92687c59', '93b353f2', '93edfe2e', '9554a50b', '99abe2bb', '99ea62f3', '9b01374f', '9b23e8ee', '9b4001e4', '9c5ef70c', '9ce586dd', '9d29771f', '9d4e7b25', '9de5e594', '9e34ea74', '9e4c8c7b', '9e6b7fb5', '9ed8f6da', '9ee1c98c', 'a0faea5d', 'a1192f43', 'a16a373e', 'a1bbe385', 'a1e4395d', 'a29c5338', 'a2df0760', 'a44b10dc', 'a52b92d5', 'a592d54e', 'a5be6304', 'a5e9da97', 'a6d66e51', 'a76029ee', 'a7640a16', 'a8876db3', 'a8a78786', 'a8efe47b', 'ab3136ba', 'ab4ec3a4', 'abc5811c', 'ac92046e', 'acf5c23f', 'ad148f58', 'ad2fc29c', 'b012cd7f', 'b120f2ac', 'b1d5101d', 'b2dba42b', 'b2e5b0f1', 'b5053438', 'b74258a0', 'b7530680', 'b7dc8128', 'b80e5e84', 'b88f38da', 'bb3e370b', 'bbfe0445', 'bc8f2793', 'bcceccc6', 'bd612267', 'bd701df8', 'bdf49a58', 'beb0a7b9', 'bfc77bd6', 'c0415e5c', 'c189aaf2', 'c1cac9a2', 'c277e121', 'c2baf0bd', 'c51d8688', 'c54cf6c5', 'c58186bf', 'c6971acf', 'c7128948', 'c74f40cd', 'c7f7f0e1', 'c7fe2a55', 'c952eb01', 'ca11f653', 'cb1178ad', 'cb6010f8', 'cc5087a3', 'cdd22e43', 'cf7638f3', 'cf82af56', 'cfbd47c8', 'd02b7a8e', 'd06f75b5', 'd122731b', 'd185d3ea', 'd2278a3b', 'd2659ab4', 'd2e9262e', 'd3268efa', 'd3640339', 'd38c2fd7', 'd3f1e122', 'd45ed6a1', 'd51b1749', 'd88ca108', 'd88e8f25', 'd9c005dd', 'daac11b0', 'db02c830', 'dcaede90', 'dcb1663e', 'dcb55a27', 'de26c3a6', 'df4940d3', 'df4fe8b6', 'e04fb33d', 'e080a381', 'e37a2b78', 'e3ff61fb', 'e4d32835', 'e4f1efe6', 'e5734469', 'e57dd7af', 'e5c9df6f', 'e64e2cfd', 'e694a35b', 'e720d930', 'e7561dd2', 'e79f3763', 'e7e44842', 'e9c52111', 'ea296733', 'ea321fb1', 'eb2c19cd', 'ec138c1c', 'ecaab346', 'ecc36b7f', 'ecc6157f', 'f28c589a', 'f32856e4', 'f3cd5473', 'f50fc6c1', 'f54238ee', 'f56e0afc', 'f5b8c21a', 'f6947f54', 'f71c4741', 'f7e47413', 'f806dc10', 'f93fc684', 'fbaf3456', 'fcfdffb6', 'fd20ea40']\n"
     ]
    }
   ],
   "source": [
    "EVENT_IDS = sorted(list(set(train_raw['event_id']) | set(test_raw['event_id'])))\n",
    "_log(f'{len(EVENT_IDS)} EVENT_IDS={EVENT_IDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bottle Filler (Activity)         112223\n",
       "Scrub-A-Dub                       96619\n",
       "Chow Time                         93142\n",
       "Sandcastle Builder (Activity)     82132\n",
       "Fireworks (Activity)              61032\n",
       "Bug Measurer (Activity)           59886\n",
       "Bubble Bath                       55264\n",
       "Dino Drink                        53989\n",
       "Dino Dive                         49368\n",
       "Crystals Rule                     46867\n",
       "Chicken Balancer (Activity)       45874\n",
       "All Star Sorting                  45863\n",
       "Flower Waterer (Activity)         43819\n",
       "Happy Camel                       39806\n",
       "Pan Balance                       38649\n",
       "Watering Hole (Activity)          35367\n",
       "Air Show                          27119\n",
       "Egg Dropper (Activity)            25941\n",
       "Leaf Leader                       25574\n",
       "Cauldron Filler (Assessment)      23440\n",
       "Mushroom Sorter (Assessment)      21962\n",
       "Bird Measurer (Assessment)        20086\n",
       "Cart Balancer (Assessment)        19235\n",
       "Chest Sorter (Assessment)         17904\n",
       "Crystal Caves - Level 2            1450\n",
       "Welcome to Lost Lagoon!            1441\n",
       "Magma Peak - Level 2               1434\n",
       "Tree Top City - Level 2            1277\n",
       "Crystal Caves - Level 3            1101\n",
       "Crystal Caves - Level 1             925\n",
       "Magma Peak - Level 1                903\n",
       "Tree Top City - Level 1             901\n",
       "Tree Top City - Level 3             842\n",
       "Ordering Spheres                    783\n",
       "Slop Problem                        552\n",
       "Lifting Heavy Things                535\n",
       "Balancing Act                       528\n",
       "Pirate's Tale                       463\n",
       "Honey Cake                          442\n",
       "12 Monkeys                          430\n",
       "Costume Box                         422\n",
       "Rulers                              320\n",
       "Heavy, Heavier, Heaviest            281\n",
       "Treasure Map                        223\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TITLES = test_raw['title'].unique()\n",
    "test_raw['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Game          572260\n",
       "Activity      466274\n",
       "Assessment    102627\n",
       "Clip           15253\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TYPES = test_raw['type'].unique()\n",
    "test_raw['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAGMAPEAK       511291\n",
       "TREETOPCITY     332295\n",
       "CRYSTALCAVES    311387\n",
       "NONE              1441\n",
       "Name: world, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORLDS = test_raw['world'].unique()\n",
    "test_raw['world'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_raw['ed.source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_raw['ed.identifier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 train_raw type=[2000, 2010, 2020, 2025, 2030, 2035, 2040, 2050, 2060, 2070, 2075, 2080, 2081, 2083, 3010, 3020, 3021, 3110, 3120, 3121, 4010, 4020, 4021, 4022, 4025, 4030, 4031, 4035, 4040, 4045, 4050, 4070, 4080, 4090, 4095, 4100, 4110, 4220, 4230, 4235, 5000, 5010]\n"
     ]
    }
   ],
   "source": [
    "vs = sorted(train_raw['event_code'].unique())\n",
    "_log(f'{len(vs)} train_raw type={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_timestamp(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "\n",
    "train_raw = _transform_timestamp(train_raw)\n",
    "test_raw = _transform_timestamp(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _set_string_type(df, cols):\n",
    "    df[cols] = df[cols].fillna(NAN).astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "cols = ['event_code', 'timestamp']\n",
    "train_raw = _set_string_type(train_raw, cols=cols)\n",
    "test_raw = _set_string_type(test_raw, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _sort_it(df):\n",
    "    return df.sort_values(by=['installation_id', 'timestamp'])\n",
    "\n",
    "\n",
    "#train_raw = _sort_it(train_raw)\n",
    "#test_raw = _sort_it(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple accuracy groups per installation id\n",
    "In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "installation_id\n",
       "0006a69f    3\n",
       "0006c192    3\n",
       "00129856    1\n",
       "001d0ed0    3\n",
       "00225f67    1\n",
       "           ..\n",
       "ff9305d7    2\n",
       "ff9715db    4\n",
       "ffc90c32    2\n",
       "ffd2871d    1\n",
       "ffeb0b1b    2\n",
       "Name: accuracy_group, Length: 3614, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = train_raw[train_raw[TARGET].notna()].groupby('installation_id')[TARGET].nunique()\n",
    "vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split not by time\n",
    "Both train and test sets span the same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_raw[timestamp] is from 2019-07-23 14:38:25.256000+00:00 to 2019-10-14 21:57:26.930000+00:00\n",
      "test_raw[timestamp] is from 2019-07-24 00:04:25.361000+00:00 to 2019-10-14 21:00:34.858000+00:00\n"
     ]
    }
   ],
   "source": [
    "_log(f'train_raw[timestamp] is from {train_raw.timestamp.min()} to {train_raw.timestamp.max()}')\n",
    "_log(f'test_raw[timestamp] is from {test_raw.timestamp.min()} to {test_raw.timestamp.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7734558 entries, 0 to 7734557\n",
      "Data columns (total 12 columns):\n",
      "event_id           object\n",
      "game_session       object\n",
      "timestamp          object\n",
      "event_data         object\n",
      "installation_id    object\n",
      "event_count        int64\n",
      "event_code         object\n",
      "game_time          int64\n",
      "title              object\n",
      "type               object\n",
      "world              object\n",
      "accuracy_group     float64\n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 767.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-08-06 05:22:41.147000+00:00',\n",
       " '2019-08-06 05:36:51.915000+00:00',\n",
       " '2019-08-06 05:38:16.835000+00:00',\n",
       " '2019-08-06 20:35:25.648000+00:00',\n",
       " '2019-08-06 20:50:35.426000+00:00']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = train_raw[(train_raw['installation_id'] == '0006a69f') & (train_raw[TARGET].notna())].groupby(['game_session', 'title', TARGET])['timestamp'].max().values\n",
    "vs = sorted(vs) \n",
    "vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio denominators={'game_time_p50_game_session': array([160867]), 'game_time_p50_game_sessiontype': array([160867]), 'game_time_p50_game_sessionworld': array([160867]), 'game_time_p50_game_sessiontitle': array([160867]), 'event_count_p50_game_session': array([134]), 'event_count_p50_game_sessiontype': array([134]), 'event_count_p50_game_sessionworld': array([134]), 'event_count_p50_game_sessiontitle': array([134])}\n"
     ]
    }
   ],
   "source": [
    "def _key(s):\n",
    "    return re.sub(r'[\\W\\s]', '', s).lower()\n",
    "\n",
    "\n",
    "def _timestamp_cutoffs(df, TARGET):\n",
    "    res = df[df[TARGET].notna()].groupby(['game_session', 'title', TARGET])['timestamp'].max()\n",
    "    res = sorted(res.values)\n",
    "    return res\n",
    "\n",
    "    \n",
    "def _target_variable(df, TARGET):\n",
    "    vs = df[TARGET].copy().dropna().unique()\n",
    "    assert len(set(vs)) == 1\n",
    "    return vs[0]\n",
    "    \n",
    "    \n",
    "def _game_session_stats(df, col, suffix, denominators={}):\n",
    "    res = {}\n",
    "    groups = [\n",
    "        ['game_session'],\n",
    "        ['game_session', 'type'],\n",
    "        ['game_session', 'world'],\n",
    "        ['game_session', 'title']\n",
    "    ]\n",
    "    for g in groups:\n",
    "        vs = df.groupby(g)[col].transform('max')\n",
    "        qs = vs.quantile([0.25, 0.5, 0.75], interpolation='lower').to_numpy()\n",
    "        g_key = _key(''.join(g))\n",
    "        k = f'{col}_p50_{g_key}{suffix}'\n",
    "        res[k] = np.int32([qs[1]])\n",
    "        if k in denominators:\n",
    "            ratio_key = f'{col}_p50_{g_key}_ratio{suffix}'\n",
    "            res[ratio_key] = np.float32([res[k][0] / denominators[k][0]])\n",
    "        #res[f'{col}_max_{k}{suffix}'] = np.int32([vs.max()])\n",
    "        #res[f'{col}_std_{k}{suffix}'] = np.float32([vs.std(ddof=0)])\n",
    "        #res[f'{col}_iqr_{k}{suffix}'] = np.int32([qs[2] - qs[0]])\n",
    "    return res\n",
    "\n",
    "\n",
    "def _count(df, col, values, suffix):\n",
    "    res = {}\n",
    "    for v in values:\n",
    "        res[f'{col}_{_key(v)}{suffix}'] = np.int32([0])\n",
    "    \n",
    "    if len(values) != 0:\n",
    "        tmp = df.groupby([col], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'{col}_{_key(row[0])}{suffix}'] = np.int32([row[1]])\n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def _event_id_features(df, EVENT_IDS, TITLES, TYPES, WORLDS, suffix):\n",
    "    res = {}\n",
    "    # initialize counts\n",
    "    for eid in EVENT_IDS:\n",
    "        res[f'eid_{eid}{suffix}'] = np.int32([0])      \n",
    "        for t in TYPES:\n",
    "            res[f'eid_{eid}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "        \n",
    "        for t in WORLDS:\n",
    "            res[f'eid_{eid}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "            \n",
    "        for t in TITLES:\n",
    "            res[f'eid_{eid}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "                      \n",
    "    tmp = df.groupby(['event_id'], as_index=False).count()\n",
    "    for row in tmp.itertuples(index=False):\n",
    "        res[f'eid_{row[0]}{suffix}'] = np.int32([row[1]])\n",
    "        \n",
    "    if len(TYPES) != 0:\n",
    "        tmp = df.groupby(['event_id', 'type'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'eid_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "\n",
    "    if len(WORLDS) != 0:\n",
    "        tmp = df.groupby(['event_id', 'world'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'eid_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "\n",
    "    if len(TITLES) != 0:\n",
    "        tmp = df.groupby(['event_id', 'title'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'eid_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def _event_code_features(df, EVENT_CODES, TITLES, TYPES, WORLDS, suffix):\n",
    "    res = {}\n",
    "    for code in EVENT_CODES:\n",
    "        res[f'event_{code}{suffix}'] = np.int32([0])      \n",
    "        for t in TYPES:\n",
    "            res[f'event_{code}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "        \n",
    "        for t in WORLDS:\n",
    "            res[f'event_{code}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "            \n",
    "        for t in TITLES:\n",
    "            res[f'event_{code}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "        \n",
    "    tmp = df.groupby(['event_code'], as_index=False).count()\n",
    "    for row in tmp.itertuples(index=False):\n",
    "        res[f'event_{row[0]}{suffix}'] = np.int32([row[1]])\n",
    "        \n",
    "    if len(TYPES) != 0:\n",
    "        tmp = df.groupby(['event_code', 'type'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'event_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "\n",
    "    if len(WORLDS) != 0:\n",
    "        tmp = df.groupby(['event_code', 'world'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'event_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "\n",
    "    if len(TITLES) != 0:\n",
    "        tmp = df.groupby(['event_code', 'title'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'event_{row[0]}_{_key(row[1])}{suffix}'] = np.int32([row[2]])\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def _event_data_features(df, suffix):\n",
    "    res = {}\n",
    "    res[f'ed_duration{suffix}'] = np.int32(df['ed.duration'].fillna(0).max())\n",
    "    res[f'ed_total_duration{suffix}'] = np.int32(df['ed.total_duration'].fillna(0).max())\n",
    "    res[f'ed_level{suffix}'] = np.int32(df['ed.level'].fillna(0).max())\n",
    "    res[f'ed_round{suffix}'] = np.int32(df['ed.round'].fillna(0).max())\n",
    "    res[f'ed_correct{suffix}'] = np.int32(df['ed.correct'].fillna(0).max())\n",
    "    res[f'ed_misses{suffix}'] = np.int32(df['ed.misses'].fillna(0).max())\n",
    "    res[f'ed_weight{suffix}'] = np.int32(df['ed.weight'].fillna(0).max())\n",
    "    res[f'ed_source_resources{suffix}'] = np.int32([sum(df['ed.source'] == 'resources')])\n",
    "    res[f'ed_source_right{suffix}'] = np.int32([sum(df['ed.source'] == 'right')])\n",
    "    res[f'ed_source_left{suffix}'] = np.int32([sum(df['ed.source'] == 'left')])\n",
    "    res[f'ed_source_scale{suffix}'] = np.int32([sum(df['ed.source'] == 'scale')])\n",
    "    res[f'ed_source_middle{suffix}'] = np.int32([sum(df['ed.source'] == 'middle')])\n",
    "    res[f'ed_source_heaviest{suffix}'] = np.int32([sum(df['ed.source'] == 'Heaviest')])\n",
    "    res[f'ed_source_heavy{suffix}'] = np.int32([sum(df['ed.source'] == 'Heavy')])\n",
    "    res[f'ed_source_lightest{suffix}'] = np.int32([sum(df['ed.source'] == 'Lightest')])\n",
    "    n = 0\n",
    "    for i in range(1, 13):\n",
    "        n += sum(df['ed.source'] == str(i))\n",
    "    res[f'ed_source_numbered{suffix}'] = np.int32([n])\n",
    "    res[f'ed_id_dot{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Dot_', regex=False))])\n",
    "    res[f'ed_id_buddy{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Buddy_', regex=False))])\n",
    "    res[f'ed_id_cleo{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Cleo_', regex=False))])\n",
    "    res[f'ed_id_mom{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Mom_', regex=False))])\n",
    "    res[f'ed_id_sid{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('sid_', regex=False))])\n",
    "    positives = {'Dot_SoCool', 'Dot_GreatJob', 'ohWow', 'wowSoCool', 'thatLooksSoCool', 'tub_success', \n",
    "                 'water_success', 'soap_success', 'Dot_Amazing', 'Dot_WhoaSoCool', 'Dot_ThatsIt', 'youDidIt_1305',\n",
    "                 'SFX_completedtask', 'Cleo_AmazingPowers', 'RIGHTANSWER1', 'Dot_Awesome', 'greatJob_1306', 'YouDidIt',\n",
    "                 'RIGHTANSWER3', 'RIGHTANSWER2', 'INSTRCOMPLETE', 'AWESOME', 'WayToGoTeam', 'Dot_NiceWorkAllMatch',\n",
    "                 'GreatFlying', 'WeDidItOneRoundLeft', 'Cleo_AweOfYourSkills', 'Dot_NiceWork'}\n",
    "    n_pos = 0\n",
    "    for p in positives:\n",
    "        n_pos += sum(df['ed.identifier'].str.contains(p, regex=False))\n",
    "    res[f'ed_id_positive{suffix}'] = np.int32([n_pos])\n",
    "    negatives = {'Dot_Uhoh', 'Dot_UhOh', 'Dot_NeedTryAgain', 'IncorrectTooHeavy', 'Dot_GoLower', 'Buddy_TryDifferentNest',\n",
    "                 'Cleo_BowlTooLight', 'Dot_GoHigher', 'Dot_SoLow', 'Dot_SoHigh', 'Dot_WhoopsTooShort', 'IncorrectTooLight',\n",
    "                 'NOT_THAT_HEAVY', 'Dot_UhOhTooTall', 'ADD_MORE_WEIGHT', 'wrong1', 'tryAgain1', 'Dot_TryWeighingAgain',\n",
    "                 'Cleo_RememberHeavierBowl', 'Dot_Whoops', 'Dot_NotBalanced', 'Mom_TooManyContainers',\n",
    "                 'WrongOver', 'Mom_TooMuchWater', 'Dot_ThatBucketNotRight', 'Dot_TryAgain', 'wrongFewer', 'WrongBetweenCliff',\n",
    "                 'Mom_NeedMoreContainers', 'Dot_Try', 'Dot_HmTooSmall'}\n",
    "    n_neg = 1\n",
    "    for ne in negatives:\n",
    "        n_neg += sum(df['ed.identifier'].str.contains(ne, regex=False))\n",
    "    res[f'ed_id_negative{suffix}'] = np.int32([n_neg])\n",
    "    res[f'ed_id_positive_ratio{suffix}'] = np.float32([n_pos / n_neg])\n",
    "    return res\n",
    "    \n",
    "\n",
    "def _features_map(df, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators, suffix=''):\n",
    "    res = {}\n",
    "    cols = ['game_time', 'event_count']\n",
    "    for col in cols:\n",
    "        res.update(_game_session_stats(df, col, suffix, denominators=denominators))\n",
    "    \n",
    "    res.update(_count(df, col='type', values=TYPES, suffix=suffix))\n",
    "    res.update(_count(df, col='world', values=WORLDS, suffix=suffix))\n",
    "    res.update(_count(df, col='title', values=TITLES, suffix=suffix))\n",
    "    res.update(_event_code_features(df, EVENT_CODES, TITLES=TITLES, TYPES=TYPES, WORLDS=WORLDS, suffix=suffix))\n",
    "    res.update(_event_id_features(df, EVENT_IDS, TITLES=TITLES, TYPES=TYPES, WORLDS=WORLDS, suffix=suffix))\n",
    "    #res.update(_event_data_features(df, suffix))\n",
    "    return res\n",
    "\n",
    "\n",
    "def _features(df, installation_id, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators):\n",
    "    res = {}\n",
    "    if TARGET in df.columns:\n",
    "        res[TARGET] = np.int16([_target_variable(df, TARGET)])\n",
    "    res['installation_id'] = [installation_id]    \n",
    "    res.update(_features_map(df, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators))\n",
    "    return pd.DataFrame.from_dict(res)\n",
    "\n",
    "\n",
    "def _preprocess(raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators):\n",
    "    res = pd.DataFrame()\n",
    "    iids = raw['installation_id'].unique()\n",
    "    prev_len = None\n",
    "    for iid in tqdm(iids):\n",
    "        whole = raw[raw['installation_id'] == iid]\n",
    "        dfs = []\n",
    "        if TARGET in whole.columns:\n",
    "            # train set: each installation id may contribute one or more examples.\n",
    "            prev = None\n",
    "            for curr in _timestamp_cutoffs(whole, TARGET):\n",
    "                if prev is not None:\n",
    "                    df = whole[(whole['timestamp'] > prev) & (whole['timestamp'] <= curr)]\n",
    "                else:\n",
    "                    df = whole[whole['timestamp'] <= curr]\n",
    "                dfs.append(df)\n",
    "                prev = curr\n",
    "        else:\n",
    "            # test set: each installation id contributes one example.\n",
    "            dfs.append(whole)\n",
    "        for df in dfs:\n",
    "            ex = _features(df, iid, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators)\n",
    "            prev_len = len(ex.columns) if prev_len is None else prev_len\n",
    "            assert len(ex.columns) == prev_len\n",
    "            prev_len = len(ex.columns)\n",
    "            res = pd.concat([res, ex], ignore_index=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "denominators = {}\n",
    "cols = ['game_time', 'event_count']\n",
    "for col in cols:\n",
    "    denominators.update(_game_session_stats(train_raw, col, suffix=''))\n",
    "_log(f'ratio denominators={denominators}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                         | 0/3614 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 1/3614 [00:07<7:32:15,  7.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 2/3614 [00:11<6:37:22,  6.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 3/3614 [00:13<5:11:12,  5.17s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 4/3614 [00:21<5:48:13,  5.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 5/3614 [00:22<4:36:21,  4.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 6/3614 [00:24<3:47:00,  3.78s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 7/3614 [00:37<6:27:32,  6.45s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 8/3614 [00:45<7:04:09,  7.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                               | 9/3614 [00:51<6:42:39,  6.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|▏                                             | 10/3614 [00:56<6:04:35,  6.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|▏                                             | 11/3614 [00:59<5:13:36,  5.22s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-055c6bb746b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# budget of 5 seconds per iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_CODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_IDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTITLES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWORLDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenominators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-559c5b1a16cd>\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhole\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_CODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_IDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTITLES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWORLDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenominators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m             \u001b[0mprev_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprev_len\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mprev_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprev_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-559c5b1a16cd>\u001b[0m in \u001b[0;36m_features\u001b[1;34m(df, installation_id, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'installation_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minstallation_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_features_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_CODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEVENT_IDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTITLES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWORLDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenominators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1203\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# from BlockManager perspective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             val = sanitize_array(\n\u001b[1;32m--> 323\u001b[1;33m                 \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             )\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;31m# we will try to copy be-definition here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[1;34m(arr, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;31m# Take care in creating object arrays (but iterators are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;31m# supported):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         if is_object_dtype(dtype) and (\n\u001b[0m\u001b[0;32m    788\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\feng\\workspace\\kaggle-data-science-bowl-2019\\venv\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_object_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_is_dtype_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# budget of 4 seconds per iteration\n",
    "train = _preprocess(train_raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info(max_cols=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train.notna().all(axis=None)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _preprocess(test_raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, denominators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info(max_cols=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert test.notna().all(axis=None)\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('train.parquet')\n",
    "test.to_parquet('test.parquet')\n",
    "_log(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Approach: Stacking two models\n",
    "1. Binary classification - was the assessment solved or not?\n",
    "1. Regression on the number of attempts taken to solve the assessment\n",
    "\n",
    "Reason: `accuracy_group` labels '1', '2' and '3' are ordinal but not '0'. See https://www.kaggle.com/c/data-science-bowl-2019/discussion/124836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_transform(df, cols):\n",
    "    df[cols] = np.float32(np.log(df[cols] + 1))\n",
    "\n",
    "\n",
    "#cols = list(set(test.columns.values) - {'installation_id'})\n",
    "#_log_transform(train, cols)\n",
    "#_log_transform(test, cols)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _scaling(dfs, cols, scaler=None):\n",
    "    scaler = sklearn.preprocessing.RobustScaler() if scaler is None else scaler\n",
    "    scaler.fit(dfs[0][cols])\n",
    "    for df in dfs:\n",
    "        df[cols] = np.float32(scaler.transform(df[cols]))\n",
    "        assert df.notna().all(axis=None)\n",
    "\n",
    "\n",
    "#scaler = sklearn.preprocessing.PowerTransformer()\n",
    "cols = list(set(test.columns.values) - {'installation_id'})\n",
    "_scaling([train, test], cols)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('train_scaled.parquet')\n",
    "test.to_parquet('test_scaled.parquet')\n",
    "_log(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "[KS Test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_parquet('train_scaled.parquet')\n",
    "test = pd.read_parquet('test_scaled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_features(df1, df2, features, alpha):\n",
    "    res = []\n",
    "    for f in tqdm(features):\n",
    "        if ks_2samp(df1[f], df2[f]).pvalue > alpha:\n",
    "            res.append(f)\n",
    "    return res\n",
    "\n",
    "\n",
    "ALPHA = float('1e-02')\n",
    "features = set(test.columns.values) - {'installation_id'}\n",
    "PREDICTORS = _select_features(train, test, features, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = sorted(list(features - set(PREDICTORS)))\n",
    "PREDICTORS = sorted(PREDICTORS)\n",
    "_log(f'alpha={ALPHA}, keep {len(PREDICTORS)}/{len(features)} features, drop {len(dropped)} features.\\nkeep={PREDICTORS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log(f'drop={dropped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_solved'] = -1\n",
    "train['solved_attempts'] = -1\n",
    "train.loc[train[TARGET] == 0, ['is_solved']] = 0\n",
    "train.loc[train[TARGET] != 0, ['is_solved']] = 1\n",
    "train.loc[train[TARGET] == 3, ['solved_attempts']] = 1\n",
    "train.loc[train[TARGET] == 2, ['solved_attempts']] = 2\n",
    "train.loc[train[TARGET] == 1, ['solved_attempts']] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify whether assessment was solved or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_cls = train['is_solved']\n",
    "x_train_cls = train[PREDICTORS]\n",
    "model = lgb.LGBMClassifier(n_estimators=10000, reg_alpha=1, objective='binary')\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.001],\n",
    "    'model__num_leaves': [8],\n",
    "    'model__min_child_samples': [40],\n",
    "    'model__colsample_bytree': [0.01]\n",
    "}\n",
    "cls = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring='f1')\n",
    "#cv.fit(x_train, y_train, model__early_stopping_rounds=200, model__verbose=500)\n",
    "cls.fit(x_train_cls, y_train_cls)\n",
    "assert cls.best_estimator_['model'].n_classes_ == 2\n",
    "_log(f\"\"\"F1 LGBMClassifier\n",
    "best_score_={cls.best_score_:.5f}\n",
    "best_params_={cls.best_params_}\n",
    "n_features={cls.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(cls.best_estimator_['model'], max_num_features=100, figsize=(10, 30), title='Classification feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(n_estimators=4000, max_features='log2')\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__max_depth': [4],\n",
    "    'model__min_samples_leaf': [40]\n",
    "}\n",
    "rfc = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring='f1')\n",
    "rfc.fit(x_train_cls, y_train_cls)\n",
    "assert rfc.best_estimator_['model'].n_classes_ == 2\n",
    "_log(f\"\"\"F1 RandomForestClassifier\n",
    "best_score_={rfc.best_score_:.5f}\n",
    "best_params_={rfc.best_params_}\n",
    "n_features={rfc.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on the number of attempts to solve the assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(y, y_pred):\n",
    "    return sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "SCORING = make_scorer(_rmse, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmp = train[train['is_solved'] == 1]\n",
    "y_train = tmp['solved_attempts']\n",
    "x_train = tmp[PREDICTORS]\n",
    "model = lgb.LGBMRegressor(n_estimators=10000, reg_alpha=1)\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.001],\n",
    "    'model__num_leaves': [16],\n",
    "    'model__min_child_samples': [40],\n",
    "    'model__colsample_bytree': [0.01]\n",
    "}\n",
    "cv = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring=SCORING)\n",
    "#cv.fit(x_train, y_train, model__early_stopping_rounds=200, model__verbose=500)\n",
    "cv.fit(x_train, y_train)\n",
    "_log(f\"\"\"RMSE LGBMRegressor\n",
    "best_score_={cv.best_score_:.5f}\n",
    "best_params_={cv.best_params_}\n",
    "n_features={cv.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metric only works with early stopping rounds\n",
    "#lgb.plot_metric(cv.best_estimator_['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(cv.best_estimator_['model'], max_num_features=100, figsize=(10, 30), title='Regression feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestRegressor(n_estimators=4000, max_features='log2')\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__max_depth': [4],\n",
    "    'model__min_samples_leaf': [40]\n",
    "}\n",
    "rfr = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring=SCORING)\n",
    "rfr.fit(x_train, y_train)\n",
    "_log(f\"\"\"RMSE RandomForestRegressor\n",
    "best_score_={rfr.best_score_:.5f}\n",
    "best_params_={rfr.best_params_}\n",
    "n_features={rfr.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict out of fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _is_solved(score):\n",
    "    if score >= 0.7:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _solved_attempts(score):\n",
    "    if score >= 1.6:\n",
    "        return 3\n",
    "    if score >= 1.54:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "\n",
    "def _predict(df, classifiers, regressors):\n",
    "    res = df[['installation_id']].copy()\n",
    "    res[TARGET] = np.nan\n",
    "    x_cls = df[PREDICTORS]\n",
    "    res['is_solved'] = 0\n",
    "    for c, w, name in classifiers:\n",
    "        col = f'is_solved_{name}'\n",
    "        res[col] = c.predict_proba(x_cls)[:,1]\n",
    "        res['is_solved'] += res[col] * w\n",
    "    \n",
    "    res['is_solved'] = np.int16(res['is_solved'].map(_is_solved))\n",
    "    iids = set(res[res['is_solved'] == 1]['installation_id'].values)\n",
    "    cols = ['installation_id'] + PREDICTORS\n",
    "    tmp = df[df['installation_id'].isin(iids)][cols].copy()\n",
    "    x = tmp[PREDICTORS]\n",
    "    cols = ['installation_id', 'solved_attempts_raw', 'solved_attempts']\n",
    "    tmp['solved_attempts_raw'] = 0\n",
    "    for r, w, name in regressors:\n",
    "        col = f'solved_attempts_{name}'\n",
    "        cols.append(col)\n",
    "        tmp[col] = r.predict(x)\n",
    "        tmp['solved_attempts_raw'] += tmp[col] * w\n",
    "        \n",
    "    tmp['solved_attempts'] = np.int16(tmp['solved_attempts_raw'].map(_solved_attempts))\n",
    "    tmp = tmp[cols]\n",
    "    res = res.merge(tmp, on='installation_id', how='left')\n",
    "    res.loc[res['is_solved'] == 0, [TARGET]] = 0\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] >= 3), [TARGET]] = 1\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] == 2), [TARGET]] = 2\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] <= 1), [TARGET]] = 3\n",
    "    assert res[TARGET].notna().all(axis=None)\n",
    "    res[TARGET] = np.int16(res[TARGET])\n",
    "    return res\n",
    "\n",
    "\n",
    "classifiers=[\n",
    "    (cls, 0.6, 'LGBMClassifier'),\n",
    "    (rfc, 0.4, 'RandomForestClassifier')\n",
    "]\n",
    "regressors=[\n",
    "    (cv, 0.6, 'LGBMRegressor'), \n",
    "    (rfr, 0.4, 'RandomForestRegressor')\n",
    "]\n",
    "oof = _predict(train, classifiers=classifiers, regressors=regressors)\n",
    "oof.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title('oof solved_attempts_LGBMRegressor')\n",
    "oof['solved_attempts_LGBMRegressor'].plot(kind='hist')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('oof solved_attempts_RandomForestRegressor')\n",
    "oof['solved_attempts_RandomForestRegressor'].plot(kind='hist')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = oof.sort_values(by=['installation_id'])\n",
    "train = train.sort_values(by=['installation_id'])\n",
    "score = cohen_kappa_score(oof[TARGET], train[TARGET], weights='quadratic')\n",
    "_log(f'oof score={score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub = _predict(test, classifiers=classifiers, regressors=regressors)\n",
    "sub = sub[['installation_id', TARGET]]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.title('test predict')\n",
    "sub[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('oof predict')\n",
    "oof[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('oof truth')\n",
    "tmp = train[TARGET].copy()\n",
    "tmp = tmp.astype(int)\n",
    "tmp.plot(kind='hist')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)\n",
    "_log(os.listdir(\".\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
