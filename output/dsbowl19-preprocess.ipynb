{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit           : None\n",
      "python           : 3.7.1.final.0\n",
      "python-bits      : 64\n",
      "OS               : Windows\n",
      "OS-release       : 7\n",
      "machine          : AMD64\n",
      "processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "byteorder        : little\n",
      "LC_ALL           : None\n",
      "LANG             : None\n",
      "LOCALE           : None.None\n",
      "\n",
      "pandas           : 0.25.3\n",
      "numpy            : 1.17.4\n",
      "pytz             : 2019.3\n",
      "dateutil         : 2.8.1\n",
      "pip              : 19.3.1\n",
      "setuptools       : 39.0.1\n",
      "Cython           : None\n",
      "pytest           : None\n",
      "hypothesis       : None\n",
      "sphinx           : None\n",
      "blosc            : None\n",
      "feather          : None\n",
      "xlsxwriter       : None\n",
      "lxml.etree       : None\n",
      "html5lib         : None\n",
      "pymysql          : None\n",
      "psycopg2         : None\n",
      "jinja2           : 2.10.3\n",
      "IPython          : 7.10.2\n",
      "pandas_datareader: None\n",
      "bs4              : None\n",
      "bottleneck       : None\n",
      "fastparquet      : None\n",
      "gcsfs            : None\n",
      "lxml.etree       : None\n",
      "matplotlib       : 3.1.2\n",
      "numexpr          : None\n",
      "odfpy            : None\n",
      "openpyxl         : None\n",
      "pandas_gbq       : None\n",
      "pyarrow          : 0.15.1\n",
      "pytables         : None\n",
      "s3fs             : None\n",
      "scipy            : 1.4.1\n",
      "sqlalchemy       : None\n",
      "tables           : None\n",
      "xarray           : None\n",
      "xlrd             : None\n",
      "xlwt             : None\n",
      "xlsxwriter       : None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "from math import sqrt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log(str):\n",
    "    os.system(f'echo \\\"{str}\\\"')\n",
    "    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ROOT = '../input/data-science-bowl-2019'\n",
    "JOIN_KEY = ['installation_id', 'game_session', 'title']\n",
    "TARGET = 'accuracy_group'\n",
    "FEATURES = {\n",
    "    'event_id', \n",
    "    'game_session', \n",
    "    'timestamp', \n",
    "    'installation_id', \n",
    "    'event_count',\n",
    "    'event_code', \n",
    "    'game_time', \n",
    "    'title', \n",
    "    'type', \n",
    "    'world',\n",
    "    'event_data'\n",
    "}\n",
    "EVENT_CODES = ['2000', '2010', '2020', '2025', '2030', '2035', '2040', '2050', '2060', '2070', '2075', '2080', '2081', '2083', '3010', '3020', '3021', '3110', '3120', '3121', '4010', '4020', '4021', '4022', '4025', '4030', '4031', '4035', '4040', '4045', '4050', '4070', '4080', '4090', '4095', '4100', '4110', '4220', '4230', '4235', '5000', '5010']\n",
    "SEED = 31\n",
    "FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init():\n",
    "    # Characters such as empty strings '' or numpy.inf are considered NA values\n",
    "    pd.set_option('use_inf_as_na', True)\n",
    "    pd.set_option('display.max_columns', 999)\n",
    "    pd.set_option('display.max_rows', 999)\n",
    "    \n",
    "    \n",
    "_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM_VALUES=[0.01227824739797545, 0.11239886108023578, 0.39283736155074256, 0.683864273453267, 0.13869093321463077, 0.11244356285938728, 0.23180235707395025, 0.7575924853950116, 0.1474012209602784, 0.7406551989400061, 0.6621666555152632, 0.13657852434493012, 0.5356617475477887, 0.4477509299372836, 0.41257008317805066, 0.9963436915955056, 0.0931113892655685, 0.02036645837312856, 0.9398019156431335, 0.40294581161091647]\n"
     ]
    }
   ],
   "source": [
    "RANDOM_VALUES = []\n",
    "for _ in range(4000):\n",
    "    RANDOM_VALUES.append(random.random())\n",
    "    \n",
    "_log(f'RANDOM_VALUES={RANDOM_VALUES[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/data-science-bowl-2019\\sample_submission.csv\n",
      "../input/data-science-bowl-2019\\test.csv\n",
      "../input/data-science-bowl-2019\\test.csv.zip\n",
      "../input/data-science-bowl-2019\\train.csv\n",
      "../input/data-science-bowl-2019\\train.csv.zip\n",
      "../input/data-science-bowl-2019\\train_labels.csv\n",
      "../input/data-science-bowl-2019\\train_labels.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_ROOT):\n",
    "    for filename in filenames:\n",
    "        _log(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_raw = pd.read_csv(f'{INPUT_ROOT}/train.csv', usecols=FEATURES)\n",
    "train_labels = pd.read_csv(f'{INPUT_ROOT}/train_labels.csv', usecols=JOIN_KEY + [TARGET])\n",
    "test_raw = pd.read_csv(f'{INPUT_ROOT}/test.csv', usecols=FEATURES)\n",
    "train_labels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_unlabelled_data(train_raw, train_labels):\n",
    "    return train_raw[train_raw['installation_id'].isin(train_labels['installation_id'].unique())]\n",
    "\n",
    "\n",
    "train_raw = _remove_unlabelled_data(train_raw, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _add_labels(train_raw, train_labels, on):\n",
    "    return pd.merge(train_raw, train_labels, on=on, how='left')\n",
    "\n",
    "\n",
    "train_raw = _add_labels(train_raw, train_labels, on=JOIN_KEY)\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract event data JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_columns(df1, df2):\n",
    "    \"\"\"Concatenate the columns of two pandas dataframes in the order of the operands.\n",
    "    Both dataframes must have the same number of rows.\n",
    "    \"\"\"\n",
    "    assert len(df1) == len(df2)\n",
    "    res = pd.concat([df1, df2.reindex(df1.index)], axis=1, join='inner')\n",
    "    assert len(res) == len(df1)\n",
    "    return res\n",
    "    \n",
    "\n",
    "def _extract_event_data(df, keep_cols, chunk_size=1000000):\n",
    "    res = pd.DataFrame()\n",
    "    _len = len(df)\n",
    "    for i in tqdm(range(0, _len, chunk_size)):\n",
    "        if i + chunk_size < _len:\n",
    "            chunk = df[i:i + chunk_size].copy()\n",
    "        else:\n",
    "            chunk = df[i:].copy()\n",
    "        ed = pd.io.json.json_normalize(chunk['event_data'].apply(json.loads)).add_prefix('ed.')\n",
    "        ed = ed[keep_cols].astype(np.float32)\n",
    "        chunk = _concat_columns(chunk, ed)\n",
    "        # sort=False because not all rows have same fields in event_data\n",
    "        res = pd.concat([res, chunk], ignore_index=True, sort=False)\n",
    "    # this line is too slow and OOM error!\n",
    "    #res[keep_cols] = res[keep_cols].fillna(-1).astype(np.float32)\n",
    "    assert len(df) == len(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "keep_cols = ['ed.duration', 'ed.level', 'ed.round', 'ed.correct', 'ed.misses','ed.weight', 'ed.total_duration']\n",
    "#keep_cols = ['ed.identifier', 'ed.duration', 'ed.level', 'ed.round', 'ed.correct', 'ed.misses','ed.weight', 'ed.total_duration', 'ed.source']\n",
    "train_raw = _extract_event_data(train_raw, keep_cols)\n",
    "test_raw = _extract_event_data(test_raw, keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All event ids in test set also exist in train set\n",
    "#test_set = set(test_raw['event_id'])\n",
    "#train_set = set(train_raw['event_id'])\n",
    "#vs = test_set - train_set\n",
    "#_log(f'{len(vs)} event_ids exist in test set but not train set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_IDS = sorted(list(set(train_raw['event_id']) | set(test_raw['event_id'])))\n",
    "_log(f'{len(EVENT_IDS)} EVENT_IDS={EVENT_IDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLES = test_raw['title'].unique()\n",
    "test_raw['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = test_raw['type'].unique()\n",
    "test_raw['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLDS = test_raw['world'].unique()\n",
    "test_raw['world'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_raw['ed.source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_raw['ed.identifier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = sorted(train_raw['event_code'].unique())\n",
    "_log(f'{len(vs)} train_raw type={vs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_prev = pd.to_datetime(pd.Series(['2019-08-06T05:22:41.147000000'])).astype(np.int64).values[0]\n",
    "_next = pd.to_datetime(pd.Series(['2019-08-06T05:22:41.147000001'])).astype(np.int64).values[0]\n",
    "assert _next - _prev == 1\n",
    "\n",
    "\n",
    "def _transform_timestamp(df):\n",
    "    vs = pd.to_datetime(df['timestamp'])\n",
    "    df['timestamp'] = vs\n",
    "    assert df['timestamp'].notna().all()\n",
    "    df['timestamp_int'] = vs.astype(np.int64)\n",
    "    assert df['timestamp_int'].notna().all()\n",
    "\n",
    "\n",
    "_transform_timestamp(train_raw)\n",
    "_transform_timestamp(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _set_string_type(df, cols):\n",
    "    df[cols] = df[cols].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "cols = ['event_code', 'timestamp']\n",
    "train_raw = _set_string_type(train_raw, cols=cols)\n",
    "test_raw = _set_string_type(test_raw, cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _sort_it(df):\n",
    "    return df.sort_values(by=['installation_id', 'timestamp'])\n",
    "\n",
    "\n",
    "#train_raw = _sort_it(train_raw)\n",
    "#test_raw = _sort_it(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple accuracy groups per installation id\n",
    "In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = train_raw[train_raw[TARGET].notna()].groupby('installation_id', as_index=False)[TARGET].nunique()\n",
    "vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split not by time\n",
    "Both train and test sets span the same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_log(f'train_raw[timestamp] is from {train_raw.timestamp.min()} to {train_raw.timestamp.max()}')\n",
    "#_log(f'test_raw[timestamp] is from {test_raw.timestamp.min()} to {test_raw.timestamp.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vs = train_raw[(train_raw['installation_id'] == '0006a69f') & (train_raw[TARGET].notna())].groupby(['game_session', 'title', TARGET], as_index=False)['timestamp'].max()\n",
    "#vs = sorted(vs['timestamp'].values) \n",
    "#vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.info(max_cols=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _key(s):\n",
    "    return re.sub(r'[\\W\\s]', '', s).lower()\n",
    "\n",
    "\n",
    "def _timestamp_cutoffs(df, TARGET):\n",
    "    res = df[df[TARGET].notna()].copy().groupby(['game_session', 'title', TARGET], as_index=False)['timestamp_int'].max()\n",
    "    res = sorted(res['timestamp_int'].values)\n",
    "    return res\n",
    "\n",
    "    \n",
    "def _target_variable(df, TARGET):\n",
    "    vs = df[TARGET].copy().dropna().unique()\n",
    "    assert len(set(vs)) == 1\n",
    "    return vs[0]\n",
    "    \n",
    "    \n",
    "def _game_session_stats(df, col, titles, types, worlds, suffix):\n",
    "    \"\"\"Try median-of-median approach, instead of median-of-max.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    _default = -999999\n",
    "    # initialize values\n",
    "    k = f'{col}_gamesession_p50{suffix}'\n",
    "    res[k] = np.float32([_default])\n",
    "    for w in worlds:\n",
    "        for t in types:\n",
    "            k = f'{col}_gamesession_{_key(w)}_{_key(t)}_p50{suffix}'\n",
    "            res[k] = np.float32([_default])\n",
    "\n",
    "    for t in titles:\n",
    "        k = f'{col}_gamesession_{_key(t)}_p50{suffix}' \n",
    "        res[k] = np.float32([_default])\n",
    "\n",
    "    tmp = df.groupby(['game_session'], as_index=False)[col].median()\n",
    "    k = f'{col}_gamesession_p50{suffix}'\n",
    "    if k in res and not tmp[col].isna().all():\n",
    "        v = tmp[col].median()\n",
    "        res[k] = np.float32([v])\n",
    "\n",
    "    if len(worlds) != 0 and len(types) != 0:\n",
    "        tmp = df.groupby(['game_session', 'world', 'type'], as_index=False)[col].median()\n",
    "        tmp.dropna(subset=[col], inplace=True)\n",
    "        tmp = tmp.groupby(['world', 'type'], as_index=False)[col].median()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'{col}_gamesession_{_key(row[0])}_{_key(row[1])}_p50{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.float32([row[2]])\n",
    "\n",
    "    if len(titles) != 0:\n",
    "        tmp = df.groupby(['game_session', 'title'], as_index=False)[col].median()\n",
    "        tmp.dropna(subset=[col], inplace=True)\n",
    "        tmp = tmp.groupby(['title'], as_index=False)[col].median()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'{col}_gamesession_{_key(row[0])}_p50{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.float32([row[1]])\n",
    "    \n",
    "    #qs = vs.quantile([0.25, 0.5, 0.75], interpolation='lower').to_numpy()    \n",
    "    return res\n",
    "\n",
    "\n",
    "def _count(df, col, values, suffix):\n",
    "    res = {}\n",
    "    for v in values:\n",
    "        res[f'{col}_{_key(v)}{suffix}'] = np.int32([0])\n",
    "    \n",
    "    if len(values) != 0:\n",
    "        tmp = df.groupby([col], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            res[f'{col}_{_key(row[0])}{suffix}'] = np.int32([row[1]])\n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def _event_id_features(df, event_ids, titles, types, worlds, suffix):\n",
    "    res = {}\n",
    "    # initialize counts\n",
    "    for eid in event_ids:\n",
    "        res[f'eid_{eid}{suffix}'] = np.int32([0])      \n",
    "        for w in worlds:\n",
    "            for t in types:\n",
    "                res[f'eid_{eid}_{_key(w)}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "            \n",
    "        for t in titles:\n",
    "            res[f'eid_{eid}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "                      \n",
    "    tmp = df.groupby(['event_id'], as_index=False).count()\n",
    "    for row in tmp.itertuples(index=False):\n",
    "        res[f'eid_{row[0]}{suffix}'] = np.int32([row[1]])\n",
    "        \n",
    "    if len(worlds) != 0 and len(types) != 0:\n",
    "        tmp = df.groupby(['event_id', 'world', 'type'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'eid_{row[0]}_{_key(row[1])}_{_key(row[2])}{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.int32([row[3]])\n",
    "\n",
    "    if len(titles) != 0:\n",
    "        tmp = df.groupby(['event_id', 'title'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'eid_{row[0]}_{_key(row[1])}{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.int32([row[2]])\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def _event_code_features(df, event_codes, titles, types, worlds, suffix):\n",
    "    res = {}\n",
    "    # initialize counts\n",
    "    for code in event_codes:\n",
    "        res[f'event_{code}{suffix}'] = np.int32([0])\n",
    "        for w in worlds:\n",
    "            for t in types:\n",
    "                res[f'event_{code}_{_key(w)}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "            \n",
    "        for t in titles:\n",
    "            res[f'event_{code}_{_key(t)}{suffix}'] = np.int32([0])\n",
    "        \n",
    "    tmp = df.groupby(['event_code'], as_index=False).count()\n",
    "    for row in tmp.itertuples(index=False):\n",
    "        res[f'event_{row[0]}{suffix}'] = np.int32([row[1]])\n",
    "        \n",
    "    if len(worlds) != 0 and len(types) != 0:\n",
    "        tmp = df.groupby(['event_code', 'world', 'type'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'event_{row[0]}_{_key(row[1])}_{_key(row[2])}{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.int32([row[3]])\n",
    "\n",
    "    if len(titles) != 0:\n",
    "        tmp = df.groupby(['event_code', 'title'], as_index=False).count()\n",
    "        for row in tmp.itertuples(index=False):\n",
    "            k = f'event_{row[0]}_{_key(row[1])}{suffix}'\n",
    "            if k in res:\n",
    "                res[k] = np.int32([row[2]])\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def _event_data_features(df, suffix):\n",
    "    res = {}\n",
    "    res[f'ed_duration{suffix}'] = np.int32(df['ed.duration'].fillna(0).max())\n",
    "    res[f'ed_total_duration{suffix}'] = np.int32(df['ed.total_duration'].fillna(0).max())\n",
    "    res[f'ed_level{suffix}'] = np.int32(df['ed.level'].fillna(0).max())\n",
    "    res[f'ed_round{suffix}'] = np.int32(df['ed.round'].fillna(0).max())\n",
    "    res[f'ed_correct{suffix}'] = np.int32(df['ed.correct'].fillna(0).max())\n",
    "    res[f'ed_misses{suffix}'] = np.int32(df['ed.misses'].fillna(0).max())\n",
    "    res[f'ed_weight{suffix}'] = np.int32(df['ed.weight'].fillna(0).max())\n",
    "    res[f'ed_source_resources{suffix}'] = np.int32([sum(df['ed.source'] == 'resources')])\n",
    "    res[f'ed_source_right{suffix}'] = np.int32([sum(df['ed.source'] == 'right')])\n",
    "    res[f'ed_source_left{suffix}'] = np.int32([sum(df['ed.source'] == 'left')])\n",
    "    res[f'ed_source_scale{suffix}'] = np.int32([sum(df['ed.source'] == 'scale')])\n",
    "    res[f'ed_source_middle{suffix}'] = np.int32([sum(df['ed.source'] == 'middle')])\n",
    "    res[f'ed_source_heaviest{suffix}'] = np.int32([sum(df['ed.source'] == 'Heaviest')])\n",
    "    res[f'ed_source_heavy{suffix}'] = np.int32([sum(df['ed.source'] == 'Heavy')])\n",
    "    res[f'ed_source_lightest{suffix}'] = np.int32([sum(df['ed.source'] == 'Lightest')])\n",
    "    n = 0\n",
    "    for i in range(1, 13):\n",
    "        n += sum(df['ed.source'] == str(i))\n",
    "    res[f'ed_source_numbered{suffix}'] = np.int32([n])\n",
    "    res[f'ed_id_dot{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Dot_', regex=False))])\n",
    "    res[f'ed_id_buddy{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Buddy_', regex=False))])\n",
    "    res[f'ed_id_cleo{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Cleo_', regex=False))])\n",
    "    res[f'ed_id_mom{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('Mom_', regex=False))])\n",
    "    res[f'ed_id_sid{suffix}'] = np.int32([sum(df['ed.identifier'].str.contains('sid_', regex=False))])\n",
    "    positives = {'Dot_SoCool', 'Dot_GreatJob', 'ohWow', 'wowSoCool', 'thatLooksSoCool', 'tub_success', \n",
    "                 'water_success', 'soap_success', 'Dot_Amazing', 'Dot_WhoaSoCool', 'Dot_ThatsIt', 'youDidIt_1305',\n",
    "                 'SFX_completedtask', 'Cleo_AmazingPowers', 'RIGHTANSWER1', 'Dot_Awesome', 'greatJob_1306', 'YouDidIt',\n",
    "                 'RIGHTANSWER3', 'RIGHTANSWER2', 'INSTRCOMPLETE', 'AWESOME', 'WayToGoTeam', 'Dot_NiceWorkAllMatch',\n",
    "                 'GreatFlying', 'WeDidItOneRoundLeft', 'Cleo_AweOfYourSkills', 'Dot_NiceWork'}\n",
    "    n_pos = 0\n",
    "    for p in positives:\n",
    "        n_pos += sum(df['ed.identifier'].str.contains(p, regex=False))\n",
    "    res[f'ed_id_positive{suffix}'] = np.int32([n_pos])\n",
    "    negatives = {'Dot_Uhoh', 'Dot_UhOh', 'Dot_NeedTryAgain', 'IncorrectTooHeavy', 'Dot_GoLower', 'Buddy_TryDifferentNest',\n",
    "                 'Cleo_BowlTooLight', 'Dot_GoHigher', 'Dot_SoLow', 'Dot_SoHigh', 'Dot_WhoopsTooShort', 'IncorrectTooLight',\n",
    "                 'NOT_THAT_HEAVY', 'Dot_UhOhTooTall', 'ADD_MORE_WEIGHT', 'wrong1', 'tryAgain1', 'Dot_TryWeighingAgain',\n",
    "                 'Cleo_RememberHeavierBowl', 'Dot_Whoops', 'Dot_NotBalanced', 'Mom_TooManyContainers',\n",
    "                 'WrongOver', 'Mom_TooMuchWater', 'Dot_ThatBucketNotRight', 'Dot_TryAgain', 'wrongFewer', 'WrongBetweenCliff',\n",
    "                 'Mom_NeedMoreContainers', 'Dot_Try', 'Dot_HmTooSmall'}\n",
    "    n_neg = 1\n",
    "    for ne in negatives:\n",
    "        n_neg += sum(df['ed.identifier'].str.contains(ne, regex=False))\n",
    "    res[f'ed_id_negative{suffix}'] = np.int32([n_neg])\n",
    "    res[f'ed_id_positive_ratio{suffix}'] = np.float32([n_pos / n_neg])\n",
    "    return res\n",
    "    \n",
    "    \n",
    "def _worlds_picked():\n",
    "    return ['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']\n",
    "\n",
    "\n",
    "def _titles_picked():\n",
    "    return ['Cauldron Filler (Assessment)', 'Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n",
    "            'Cart Balancer (Assessment)', 'Chest Sorter (Assessment)'\n",
    "           ]\n",
    "    \n",
    "\n",
    "def _types_picked():\n",
    "    return ['Assessment', 'Game']\n",
    "    \n",
    "        \n",
    "def _features_map(df, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS, suffix=''):\n",
    "    res = {}\n",
    "    worlds = _worlds_picked()\n",
    "    titles = _titles_picked()\n",
    "    types = _types_picked()\n",
    "    cols = ['game_time', 'event_count', 'ed.duration', 'ed.level', 'ed.round',\n",
    "            'ed.correct','ed.misses','ed.weight','ed.total_duration']\n",
    "    for col in cols:\n",
    "        res.update(_game_session_stats(df, col, titles=titles, types=types, worlds=worlds, suffix=suffix))\n",
    "    \n",
    "    res.update(_count(df, col='type', values=TYPES, suffix=suffix))\n",
    "    res.update(_count(df, col='world', values=WORLDS, suffix=suffix))\n",
    "    res.update(_count(df, col='title', values=TITLES, suffix=suffix))\n",
    "    res.update(_event_code_features(df, EVENT_CODES, titles=titles, types=types, worlds=worlds, suffix=suffix))\n",
    "    res.update(_event_id_features(df, EVENT_IDS, titles=titles, types=types, worlds=worlds, suffix=suffix))\n",
    "    #res.update(_event_data_features(df, suffix))\n",
    "    return res\n",
    "\n",
    "\n",
    "def _features(df, installation_id, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS):\n",
    "    res = {}\n",
    "    if TARGET in df.columns:\n",
    "        res[TARGET] = np.int16([_target_variable(df, TARGET)])\n",
    "    res['installation_id'] = [installation_id]    \n",
    "    res.update(_features_map(df, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS))\n",
    "    return pd.DataFrame.from_dict(res)\n",
    "\n",
    "\n",
    "def _preprocess(raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS):\n",
    "    res = pd.DataFrame()\n",
    "    raw = raw.set_index('installation_id', drop=False)\n",
    "    iids = raw['installation_id'].unique()\n",
    "    prev_len = None\n",
    "    prev_cols = None\n",
    "    rv = 0\n",
    "    for iid in tqdm(iids):\n",
    "        whole = raw.loc[[iid]].copy()  # double square brackets return a Dataframe!\n",
    "        whole = whole.set_index('timestamp_int', drop=False)\n",
    "        dfs = []\n",
    "        if TARGET in whole.columns:\n",
    "            # train set: each installation id may contribute one or more examples.\n",
    "            _prev = pd.to_datetime(pd.Series(['1999-01-01T05:22:41.147000000'])).astype(np.int64).values[0]\n",
    "            for _curr in _timestamp_cutoffs(whole, TARGET):\n",
    "                df = whole.loc[_prev + 1:_curr]\n",
    "                dfs.append(df)\n",
    "                _prev = _curr\n",
    "        else:\n",
    "            # test set: each installation id contributes one example.\n",
    "            dfs.append(whole)\n",
    "        j = -1\n",
    "        if len(dfs) > 1:\n",
    "            j = int(RANDOM_VALUES[rv])\n",
    "            rv += 1\n",
    "        for i, df in enumerate(dfs):\n",
    "            if TARGET in df.columns:\n",
    "                installation_id = f'{iid}_{i + 1}'\n",
    "            else:\n",
    "                installation_id = iid\n",
    "            ex = _features(df, installation_id, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS)\n",
    "            if TARGET in df.columns:\n",
    "                if i == j:\n",
    "                    ex['_is_val'] = 0  # validation set\n",
    "                else:\n",
    "                    ex['_is_val'] = -1\n",
    "            prev_len = len(ex.columns) if prev_len is None else prev_len\n",
    "            prev_cols = set(ex.columns) if prev_cols is None else prev_cols\n",
    "            if len(ex.columns) != prev_len:\n",
    "                _diff = set(ex.columns) - prev_cols\n",
    "                raise ValueError(f'Number of columns must be the same. Difference found={_diff}')\n",
    "            prev_len = len(ex.columns)\n",
    "            res = pd.concat([res, ex], ignore_index=True)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget of 4 seconds per iteration\n",
    "train = _preprocess(train_raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info(max_cols=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train.notna().all(axis=None)\n",
    "tmp = train.groupby(['_is_val'], as_index=False)['installation_id'].count()\n",
    "assert tmp.iloc[1]['installation_id'] >= 2000\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_raw, tmp\n",
    "gc.collect()\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _preprocess(test_raw, EVENT_CODES, EVENT_IDS, TITLES, TYPES, WORLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info(max_cols=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert test.notna().all(axis=None)\n",
    "del test_raw\n",
    "gc.collect()\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('train.parquet')\n",
    "test.to_parquet('test.parquet')\n",
    "_log(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Approach: Stacking two models\n",
    "1. Binary classification - was the assessment solved or not?\n",
    "1. Regression on the number of attempts taken to solve the assessment\n",
    "\n",
    "Reason: `accuracy_group` labels '1', '2' and '3' are ordinal but not '0'. See https://www.kaggle.com/c/data-science-bowl-2019/discussion/124836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_transform(df, cols):\n",
    "    df[cols] = np.float32(np.log(df[cols] + 1))\n",
    "\n",
    "\n",
    "#cols = list(set(test.columns.values) - {'installation_id'})\n",
    "#_log_transform(train, cols)\n",
    "#_log_transform(test, cols)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _scaling(dfs, cols, scaler=None):\n",
    "    scaler = sklearn.preprocessing.RobustScaler() if scaler is None else scaler\n",
    "    scaler.fit(dfs[0][cols])\n",
    "    for df in dfs:\n",
    "        df[cols] = np.float32(scaler.transform(df[cols]))\n",
    "        assert df.notna().all(axis=None)\n",
    "\n",
    "\n",
    "#scaler = sklearn.preprocessing.PowerTransformer()\n",
    "cols = list(set(test.columns.values) - {'installation_id', '_is_val'})\n",
    "_scaling([train, test], cols)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('train_scaled.parquet')\n",
    "test.to_parquet('test_scaled.parquet')\n",
    "_log(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "[KS Test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_parquet('train_scaled.parquet')\n",
    "test = pd.read_parquet('test_scaled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_features(df1, df2, features, alpha):\n",
    "    res = []\n",
    "    for f in tqdm(features):\n",
    "        if ks_2samp(df1[f], df2[f]).pvalue > alpha:\n",
    "            res.append(f)\n",
    "    return res\n",
    "\n",
    "\n",
    "ALPHA = 0.2\n",
    "features = set(test.columns.values) - {'installation_id', '_is_val'}\n",
    "PREDICTORS = _select_features(train, test, features, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = sorted(list(features - set(PREDICTORS)))\n",
    "PREDICTORS = sorted(PREDICTORS)\n",
    "_log(f'alpha={ALPHA}, keep {len(PREDICTORS)}/{len(features)} features, drop {len(dropped)} features.\\nkeep={PREDICTORS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log(f'drop={dropped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_solved'] = -1\n",
    "train['solved_attempts'] = -1\n",
    "train.loc[train[TARGET] == 0, ['is_solved']] = 0\n",
    "train.loc[train[TARGET] != 0, ['is_solved']] = 1\n",
    "train.loc[train[TARGET] == 3, ['solved_attempts']] = 1\n",
    "train.loc[train[TARGET] == 2, ['solved_attempts']] = 2\n",
    "train.loc[train[TARGET] == 1, ['solved_attempts']] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify whether assessment was solved or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_cls = train['is_solved']\n",
    "x_train_cls = train[PREDICTORS]\n",
    "p_split = PredefinedSplit(test_fold=train['_is_val'].values)\n",
    "model = lgb.LGBMClassifier(n_estimators=10000, reg_alpha=1, objective='binary')\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.001],\n",
    "    'model__min_child_samples': [100],\n",
    "    'model__colsample_bytree': [0.01]\n",
    "}\n",
    "cls = GridSearchCV(pipe, cv=p_split, param_grid=param_grid, scoring='f1')\n",
    "#cv.fit(x_train, y_train, model__early_stopping_rounds=200, model__verbose=500)\n",
    "cls.fit(x_train_cls, y_train_cls)\n",
    "assert cls.best_estimator_['model'].n_classes_ == 2\n",
    "_log(f\"\"\"F1 LGBMClassifier\n",
    "best_score_={cls.best_score_:.5f}\n",
    "best_params_={cls.best_params_}\n",
    "n_features={cls.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(cls.best_estimator_['model'], max_num_features=100, figsize=(10, 30), title='Classification feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _random_forest_classifier(x_train_cls, y_train_cls):\n",
    "    model = RandomForestClassifier(n_estimators=4000, max_features='log2')\n",
    "    pipe = Pipeline([('model', model)])\n",
    "    param_grid = {\n",
    "        'model__max_depth': [4],\n",
    "        'model__min_samples_leaf': [40]\n",
    "    }\n",
    "    rfc = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring='f1')\n",
    "    rfc.fit(x_train_cls, y_train_cls)\n",
    "    assert rfc.best_estimator_['model'].n_classes_ == 2\n",
    "    return rfc\n",
    "\n",
    "\n",
    "#rfc = _random_forest_classifier(x_train_cls, y_train_cls)\n",
    "#_log(f\"\"\"F1 RandomForestClassifier\n",
    "#best_score_={rfc.best_score_:.5f}\n",
    "#best_params_={rfc.best_params_}\n",
    "#n_features={rfc.best_estimator_['model'].n_features_}\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on the number of attempts to solve the assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rmse(y, y_pred):\n",
    "    return sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "SCORING = make_scorer(_rmse, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train[train['is_solved'] == 1]\n",
    "y_train = tmp['solved_attempts']\n",
    "x_train = tmp[PREDICTORS]\n",
    "p_split = PredefinedSplit(test_fold=tmp['_is_val'].values)\n",
    "\n",
    "split_df = tmp.groupby(['_is_val'], as_index=False)['installation_id'].count()\n",
    "assert split_df.iloc[1]['installation_id'] >= 1500\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = lgb.LGBMRegressor(n_estimators=10000, reg_alpha=1)\n",
    "pipe = Pipeline([('model', model)])\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.001],\n",
    "    'model__min_child_samples': [100],\n",
    "    'model__colsample_bytree': [0.1]\n",
    "}\n",
    "cv = GridSearchCV(pipe, cv=p_split, param_grid=param_grid, scoring=SCORING)\n",
    "#cv.fit(x_train, y_train, model__early_stopping_rounds=200, model__verbose=500)\n",
    "cv.fit(x_train, y_train)\n",
    "_log(f\"\"\"RMSE LGBMRegressor\n",
    "best_score_={cv.best_score_:.5f}\n",
    "best_params_={cv.best_params_}\n",
    "n_features={cv.best_estimator_['model'].n_features_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metric only works with early stopping rounds\n",
    "#lgb.plot_metric(cv.best_estimator_['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(cv.best_estimator_['model'], max_num_features=100, figsize=(10, 30), title='Regression feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _random_forest_regressor(x_train, y_train):\n",
    "    model = RandomForestRegressor(n_estimators=4000, max_features='log2')\n",
    "    pipe = Pipeline([('model', model)])\n",
    "    param_grid = {\n",
    "        'model__max_depth': [4],\n",
    "        'model__min_samples_leaf': [40]\n",
    "    }\n",
    "    rfr = GridSearchCV(pipe, cv=FOLDS, param_grid=param_grid, scoring=SCORING)\n",
    "    rfr.fit(x_train, y_train)\n",
    "    return rfr\n",
    "\n",
    "\n",
    "#rfr = _random_forest_regressor(x_train, y_train)\n",
    "#_log(f\"\"\"RMSE RandomForestRegressor\n",
    "#best_score_={rfr.best_score_:.5f}\n",
    "#best_params_={rfr.best_params_}\n",
    "#n_features={rfr.best_estimator_['model'].n_features_}\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict out of fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _is_solved(score):\n",
    "    if score >= 0.6:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _solved_attempts(score):\n",
    "    if score >= 2.5:  # 1.6\n",
    "        return 3\n",
    "    if score >= 1.5:\n",
    "        return 2\n",
    "    return 1\n",
    "\n",
    "\n",
    "def _predict(df, classifiers, regressors):\n",
    "    res = df[['installation_id']].copy()\n",
    "    res[TARGET] = np.nan\n",
    "    x_cls = df[PREDICTORS]\n",
    "    res['is_solved'] = 0\n",
    "    for c, w, name in classifiers:\n",
    "        col = f'is_solved_{name}'\n",
    "        res[col] = c.predict_proba(x_cls)[:,1]\n",
    "        res['is_solved'] += res[col] * w\n",
    "    \n",
    "    res['is_solved'] = np.int16(res['is_solved'].map(_is_solved))\n",
    "    iids = set(res[res['is_solved'] == 1]['installation_id'].values)\n",
    "    cols = ['installation_id'] + PREDICTORS\n",
    "    tmp = df[df['installation_id'].isin(iids)][cols].copy()\n",
    "    x = tmp[PREDICTORS]\n",
    "    cols = ['installation_id', 'solved_attempts_raw', 'solved_attempts']\n",
    "    tmp['solved_attempts_raw'] = 0\n",
    "    for r, w, name in regressors:\n",
    "        col = f'solved_attempts_{name}'\n",
    "        cols.append(col)\n",
    "        tmp[col] = r.predict(x)\n",
    "        tmp['solved_attempts_raw'] += tmp[col] * w\n",
    "        \n",
    "    tmp['solved_attempts'] = np.int16(tmp['solved_attempts_raw'].map(_solved_attempts))\n",
    "    tmp = tmp[cols]\n",
    "    res = res.merge(tmp, on='installation_id', how='left')\n",
    "    res.loc[res['is_solved'] == 0, [TARGET]] = 0\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] >= 3), [TARGET]] = 1\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] == 2), [TARGET]] = 2\n",
    "    res.loc[(res['is_solved'] == 1) & (res['solved_attempts'] <= 1), [TARGET]] = 3\n",
    "    assert res[TARGET].notna().all(axis=None)\n",
    "    res[TARGET] = np.int16(res[TARGET])\n",
    "    return res\n",
    "\n",
    "\n",
    "classifiers=[\n",
    "    (cls, 1, 'LGBMClassifier')\n",
    "]\n",
    "regressors=[\n",
    "    (cv, 1, 'LGBMRegressor')\n",
    "]\n",
    "oof = _predict(train, classifiers=classifiers, regressors=regressors)\n",
    "oof.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title('oof is_solved_LGBMClassifier')\n",
    "oof['is_solved_LGBMClassifier'].plot(kind='hist')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('oof solved_attempts_LGBMRegressor')\n",
    "oof['solved_attempts_LGBMRegressor'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.sort_values(by=['installation_id'], inplace=True)\n",
    "train.sort_values(by=['installation_id'], inplace=True)\n",
    "score = cohen_kappa_score(oof[TARGET], train[TARGET], weights='quadratic')\n",
    "_log(f'oof score={score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub = _predict(test, classifiers=classifiers, regressors=regressors)\n",
    "sub = sub[['installation_id', TARGET]]\n",
    "sample_sub = pd.read_csv(f'{INPUT_ROOT}/sample_submission.csv')\n",
    "assert sub['installation_id'].equals(sample_sub['installation_id'])\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.title('test predict')\n",
    "sub[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('oof predict')\n",
    "oof[TARGET].plot(kind='hist')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('oof truth')\n",
    "tmp = train[TARGET].copy()\n",
    "tmp = tmp.astype(int)\n",
    "tmp.plot(kind='hist')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)\n",
    "_log(os.listdir(\".\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
